{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/Multi_Task_NER_POS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aqdi-WDx1jW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x7l0GdpyKbv"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y63PIUi_yYOu"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download rohitr4307/ner-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2NK45lEygxf"
      },
      "outputs": [],
      "source": [
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtnNOe0CywKf"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/ner-dataset.zip','r') as zip_ref:\n",
        "  zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpaowh0sy9dX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/NER_Dataset.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzK1tJ0X0tue"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjsRm7sU2WX7"
      },
      "outputs": [],
      "source": [
        "df['Word'].iloc[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfZptOSp2eEO"
      },
      "outputs": [],
      "source": [
        "df['Tag'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac8XNFXo2eAu"
      },
      "outputs": [],
      "source": [
        "#B-gpe, B-tim, B-geo, B-org,I-geo, I-org, I-tim ---BIO Formats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "VOK_-H3p3b5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qHam1sO3xxW"
      },
      "outputs": [],
      "source": [
        "# First Step Tokenization\n",
        "# our input is already token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Token to token id"
      ],
      "metadata": {
        "id": "PulmMK7I3f0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6VkqnDKGqwg"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "df['Word'] = df['Word'].apply(ast.literal_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pos ucun Eval appyle et\n",
        "df['POS']=df['POS'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "LCFGgnFqp2JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GW4rsy__p54J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FLFW8tKYp55A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXgYs3e4Gvhd"
      },
      "outputs": [],
      "source": [
        "df['Word'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk90uLJ-HN8X"
      },
      "outputs": [],
      "source": [
        "all_tokens = [token for row in df['Word'] for token in row]\n",
        "all_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHbiD1GZHcuv"
      },
      "outputs": [],
      "source": [
        "vocab = {token: idx+1 for idx, token in enumerate(sorted(set(all_tokens)))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLAZzKzNHe0m"
      },
      "outputs": [],
      "source": [
        "vocab_size=len(vocab)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQxnoBfbHqxd"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4xNATUq6xGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbmB0LfLD0IX"
      },
      "outputs": [],
      "source": [
        "def tokens_to_ids(token_list, vocab):\n",
        "    return [vocab.get(token, 0) for token in token_list]\n",
        "\n",
        "token_ids = tokens_to_ids(all_tokens, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['token_ids'] = df['Word'].apply(lambda tokens: tokens_to_ids(tokens, vocab))"
      ],
      "metadata": {
        "id": "Y31INkJJ63mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['token_ids']"
      ],
      "metadata": {
        "id": "b7Rows6G6wb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tag token id cevirmeliyik"
      ],
      "metadata": {
        "id": "oy0doKHd3sjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tag'].dtype"
      ],
      "metadata": {
        "id": "1gqHjlsL5UMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "df['Tag'] = df['Tag'].apply(ast.literal_eval)\n"
      ],
      "metadata": {
        "id": "sw4gea_y5xlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#uniques for POS\n",
        "uniques_pos=sorted(df['POS'].explode().unique())\n",
        "pos2id={pos: idx for idx,pos in enumerate(uniques_pos)}\n",
        "id2pos={idx: pos for pos,idx in pos2id.items()}"
      ],
      "metadata": {
        "id": "n7tvOGAKsHti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniques_tag=sorted(df['Tag'].explode().unique())\n",
        "uniques_tag"
      ],
      "metadata": {
        "id": "kg_QCfxp4tRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2id = {tag: idx for idx, tag in enumerate(uniques_tag)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}"
      ],
      "metadata": {
        "id": "aH9P77Fs6CJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2id"
      ],
      "metadata": {
        "id": "OILCFxcg6OCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2tag"
      ],
      "metadata": {
        "id": "lnIgHEGU6P2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tag_ids'] = df['Tag'].apply(lambda tags: [tag2id[tag] for tag in tags])"
      ],
      "metadata": {
        "id": "3MvX_NKC6REp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pos_ids']  = df['POS'].apply(lambda tags: [pos2id[tag] for tag in tags])"
      ],
      "metadata": {
        "id": "3Gsmg3F_spNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tag_ids']"
      ],
      "metadata": {
        "id": "TEIX0yBo6iXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indi Token id leri paddin etmek lazimdiki hem imput hem output uzunlugu eyni olsun"
      ],
      "metadata": {
        "id": "u2aNJfp17EpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "cZT7h9eT7dDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['seq_len'] = df['token_ids'].apply(len)\n",
        "\n",
        "print(\"Max Length:\", df['seq_len'].max())"
      ],
      "metadata": {
        "id": "WIyDKmUk7uIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = int(df['seq_len'].quantile(0.95))"
      ],
      "metadata": {
        "id": "vCns2Qco70rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN"
      ],
      "metadata": {
        "id": "q7MBAvAM72PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos2id.keys())"
      ],
      "metadata": {
        "id": "nZsNhNkKuXUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 35\n",
        "\n",
        "df['input_ids'] = list(pad_sequences(df['token_ids'], maxlen=MAX_LEN, padding='post', value=0))\n",
        "df['label_ids'] = list(pad_sequences(df['tag_ids'], maxlen=MAX_LEN, padding='post', value=tag2id['O']))\n",
        "\n",
        "# pos ucun padding\n",
        "df['pos_label_ids']=list(pad_sequences(df['pos_ids'], maxlen=MAX_LEN, padding='post',value=pos2id['NN']))\n"
      ],
      "metadata": {
        "id": "OVdhOO_87K20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['input_ids']"
      ],
      "metadata": {
        "id": "kqFz8zSL7791"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label_ids']"
      ],
      "metadata": {
        "id": "o4XF204e7-6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pos_label_ids']"
      ],
      "metadata": {
        "id": "RrF7scgIvfva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train teste bolek demeli hem POS ucun Hem de NER ucun ayri ayri output olmalidi multi Task ucun ona gore y hissesini y_ner ve\n",
        "# y_pos kimi ayirdiqdan sonra train_test_split istifade edecem"
      ],
      "metadata": {
        "id": "hj1o1uhB8HxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = np.array(df['input_ids'].to_list(),dtype=np.int32)\n",
        "y_ner = np.array(df['label_ids'].to_list(),dtype=np.int32)\n",
        "y_pos=np.array(df['pos_label_ids'].to_list(), dtype=np.int32)\n",
        "\n",
        "# Pozitional ID-lər (0, 1, 2, ..., MAX_LEN-1)\n",
        "pos_ids = np.tile(np.arange(X.shape[1]), (X.shape[0], 1))\n",
        "\n",
        "X_train, X_test, y_ner_train, y_ner_test, y_pos_train, y_pos_test, pos_train, pos_test = train_test_split(\n",
        "    X, y_ner, y_pos, pos_ids, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Mi1zPs9V8Kjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape[0], y_ner.shape[0], y_pos.shape[0], pos_ids.shape[0]"
      ],
      "metadata": {
        "id": "IMSNfdTjxmFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X dtype:\", X.dtype)"
      ],
      "metadata": {
        "id": "8xoI5SwBP20G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow datasina cevirek"
      ],
      "metadata": {
        "id": "vhKoxeig8V2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ((X_train, pos_train), (y_ner_train, y_pos_train))\n",
        ").batch(batch_size).shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ((X_test, pos_test), (y_ner_test, y_pos_test))\n",
        ").batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "rCbMxHeY8ZUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "XI8PWEdS82so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset"
      ],
      "metadata": {
        "id": "e5WOOM7f85J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder base transformer model\n",
        "\n",
        "encoder_inputs=tf.keras.Input(shape=[None,], name='encoder_inputs')\n",
        "\n",
        "# encoder ama bu defe POS ucu\n",
        "pos_encoder_inputs=tf.keras.Input(shape=[None,], name='pos_encoder_inputs')\n",
        "\n",
        "#Embedding layer\n",
        "embed_layer=tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True, name='embed_layer')\n",
        "\n",
        "# Same Pos ucun emdedding layer\n",
        "pos_em_lay=tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=128,mask_zero=True, name='pos_em_layer')\n",
        "\n",
        "#encoder embedding\n",
        "encoder_embed=embed_layer(encoder_inputs)\n",
        "pos_encoder_embed=pos_em_lay(pos_encoder_inputs)\n",
        "sum_pos_and_input_embed=encoder_embed+pos_encoder_embed\n",
        "\n",
        "\n",
        "#Positional embed layer\n",
        "embed_size=128\n",
        "pos_embed_layer=tf.keras.layers.Embedding(MAX_LEN, embed_size)\n",
        "\n",
        "pos_encoder=tf.keras.layers.Lambda(lambda x: tf.range(start=0,limit=tf.shape(x)[1],\n",
        "                                                            delta=1))(encoder_inputs)\n",
        "\n",
        "positional_embedding=pos_embed_layer(pos_encoder)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#concat tokrn and positional embedding\n",
        "encoder_embedding=positional_embedding+sum_pos_and_input_embed\n",
        "\n",
        "#Add multihead attentioan layer\n",
        "\n",
        "num_heads=3\n",
        "encoder_attention=tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(encoder_embedding,encoder_embedding)\n",
        "encoder_attention=tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_attention+encoder_embedding)\n",
        "\n",
        "\n",
        "# add Feed Forward (Dense) layer\n",
        "ff_dim=512\n",
        "encoder_ff=tf.keras.layers.Dense(ff_dim, activation='relu')(encoder_attention)\n",
        "encoder_ff=tf.keras.layers.Dense(embed_size)(encoder_ff)\n",
        "encoder_ff=tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_ff+encoder_attention)\n",
        "\n",
        "\n",
        "#Add output layer Multi task olduguna gore hem Ner ucun Hem POS ucun ayri ayri output yazmaliyiq\n",
        "ner_output=tf.keras.layers.Dense(len(tag2id), activation='softmax', name='ner_output')(encoder_ff)\n",
        "\n",
        "pos_output=tf.keras.layers.Dense(len(pos2id), activation='softmax', name='pos_output')(encoder_ff)"
      ],
      "metadata": {
        "id": "6XyiBaVs861T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Callback and learning schedule\n",
        "learning_rate_schedule=tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9\n",
        ")\n",
        "\n",
        "early_stopping=tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "zkHz26kylJfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make model\n",
        "model=tf.keras.Model(inputs=[encoder_inputs,pos_encoder_inputs], outputs=[ner_output, pos_output])"
      ],
      "metadata": {
        "id": "BRNWUvaY_R2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)"
      ],
      "metadata": {
        "id": "BDCOjhnzl5Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model compiling\n",
        "model.compile(optimizer=optimizer,\n",
        "\n",
        "              loss= {'ner_output': 'sparse_categorical_crossentropy',\n",
        "                    'pos_output': 'sparse_categorical_crossentropy' },\n",
        "\n",
        "               metrics={\n",
        "        'ner_output': 'accuracy',\n",
        "        'pos_output': 'accuracy'}\n",
        "    )"
      ],
      "metadata": {
        "id": "hojie7gcSnGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model fitting\n",
        "model.fit(train_dataset, validation_data=valid_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "dS1FPR3nBfH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Proqnozları al\n",
        "pred_ner_probs, pred_pos_probs = model.predict(valid_dataset)\n",
        "\n",
        "# Argmax ilə ən yüksək ehtimallı etiketləri seç\n",
        "pred_ner_labels = np.argmax(pred_ner_probs, axis=-1)\n",
        "pred_pos_labels = np.argmax(pred_pos_probs, axis=-1)\n",
        "\n",
        "# İlk 2 nümunəni göstər\n",
        "for i in range(2):\n",
        "    print(f\"\\nExample {i+1}\")\n",
        "    print(f\"{'Token':15} {'True NER':10} {'Pred NER':10} {'True POS':10} {'Pred POS':10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    input_ids = X_test[i]\n",
        "    true_ner_ids = y_ner_test[i]\n",
        "    pred_ner_ids = pred_ner_labels[i]\n",
        "    true_pos_ids = y_pos_test[i]\n",
        "    pred_pos_ids = pred_pos_labels[i]\n",
        "\n",
        "    for token_id, true_ner_id, pred_ner_id, true_pos_id, pred_pos_id in zip(input_ids, true_ner_ids, pred_ner_ids, true_pos_ids, pred_pos_ids):\n",
        "        if token_id != 0:  # padding tokenləri atla\n",
        "            token = [k for k, v in vocab.items() if v == token_id][0]\n",
        "            true_ner_tag = id2tag[true_ner_id]\n",
        "            pred_ner_tag = id2tag[pred_ner_id]\n",
        "            true_pos_tag = id2pos[true_pos_id]\n",
        "            pred_pos_tag = id2pos[pred_pos_id]\n",
        "\n",
        "            print(f\"{token:15} {true_ner_tag:10} {pred_ner_tag:10} {true_pos_tag:10} {pred_pos_tag:10}\")\n"
      ],
      "metadata": {
        "id": "S8KfcbrPznKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(sentence, model, vocab, id2tag, id2pos, MAX_LEN):\n",
        "    # 1. Tokenləşdir (sadə boşluqla ayırma)\n",
        "    tokens = sentence.split()\n",
        "\n",
        "    # 2. Tokenləri id-ə çevir\n",
        "    token_ids = [vocab.get(token, 0) for token in tokens]\n",
        "\n",
        "    # 3. Padding et\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences([token_ids], maxlen=MAX_LEN, padding='post', value=0)\n",
        "\n",
        "    # 4. Pozisional id hazırla\n",
        "    pos_ids = np.tile(np.arange(MAX_LEN), (1, 1))\n",
        "\n",
        "    # 5. Modeldən proqnoz al\n",
        "    pred_ner_probs, pred_pos_probs = model.predict([input_ids, pos_ids])\n",
        "\n",
        "    # 6. Argmax et\n",
        "    pred_ner = np.argmax(pred_ner_probs, axis=-1)[0]\n",
        "    pred_pos = np.argmax(pred_pos_probs, axis=-1)[0]\n",
        "\n",
        "    # 7. Nəticələri çap et\n",
        "    print(f\"{'Token':15} {'Predicted NER':15} {'Predicted POS'}\")\n",
        "    print(\"-\" * 45)\n",
        "    for i, token in enumerate(tokens):\n",
        "        print(f\"{token:15} {id2tag.get(pred_ner[i], 'O'):15} {id2pos.get(pred_pos[i], 'NN')}\")\n",
        "\n",
        "# Nümunə istifadə:\n",
        "sentence = \"Gloria Steinem works with the National Organization for Women to promote feminism.\"\n",
        "predict_sentence(sentence, model, vocab, id2tag, id2pos, MAX_LEN)\n"
      ],
      "metadata": {
        "id": "1yZDTzrpz1va"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "Multi-Task NER/POS.ipynb",
      "authorship_tag": "ABX9TyMbINiJs9Sgd64bcC/dtB1G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}