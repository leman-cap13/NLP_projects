{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVBkHGE6bEzaMfMJyM3ySu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/transformer_anatomy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEAAdd7ue4zV"
      },
      "outputs": [],
      "source": [
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from bertviz.transformers_neuron_view import BertModel\n",
        "from bertviz.neuron_view import show\n",
        "\n",
        "model_ckpt='bert-base-uncased'\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model=BertModel.from_pretrained(model_ckpt)\n",
        "text='time flies like a arrow'\n",
        "show(model,'bert', tokenizer, text, display_mode='light', layer=0,head=8)"
      ],
      "metadata": {
        "id": "m-Xf1YS7e_TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "model_ckpt='bert-base-uncased'\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_ckpt)\n",
        "text='time flieas like an arrow'\n"
      ],
      "metadata": {
        "id": "iZQIhXsPgJc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
        "inputs.input_ids"
      ],
      "metadata": {
        "id": "FZRXUsi5h1lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import AutoConfig\n",
        "config=AutoConfig.from_pretrained(model_ckpt)\n",
        "token_embd=nn.Embedding(config.vocab_size,config.hidden_size)\n",
        "token_embd"
      ],
      "metadata": {
        "id": "numRb40MiCnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_embeds=token_embd(inputs.input_ids)\n",
        "inputs_embeds.size()"
      ],
      "metadata": {
        "id": "YgSBkvjOiCju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from math import sqrt\n",
        "\n",
        "query=key=value=inputs_embeds\n",
        "dim_k=key.size(-1)\n",
        "scores=torch.bmm(query, key.transpose(1,2))/sqrt(dim_k)\n",
        "scores.size()"
      ],
      "metadata": {
        "id": "MTM85kN4jKcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bmm bartch matmul( matrix multiplication)"
      ],
      "metadata": {
        "id": "UUps6orkjjlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.shape, key.transpose(1,2).shape"
      ],
      "metadata": {
        "id": "3PG2-BE1kZCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import softmax\n",
        "\n",
        "weights=softmax(scores, dim=-1)\n",
        "weights.sum(dim=-1)"
      ],
      "metadata": {
        "id": "93msiJC_kiMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_outputs=torch.bmm(weights, value)\n",
        "attn_outputs.shape"
      ],
      "metadata": {
        "id": "nvT-0rnJk2v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "  dim_k=key.size(-1)\n",
        "  scores=torch.bmm(query,key.transpose(1,2))/ sqrt(dim_k)\n",
        "  weights=softmax(scores,dim=-1)\n",
        "  return torch.bmm(weights,value)"
      ],
      "metadata": {
        "id": "KmYZzPFGk2sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self,embed_dim,head_dim):\n",
        "    super().__init__()\n",
        "    self.q=nn.Linear(embed_dim,head_dim)\n",
        "    self.k=nn.Linear(embed_dim, head_dim)\n",
        "    self.v=nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "  def forward(self,hidden_state):\n",
        "    attn_outputs=scaled_dot_product_attention(\n",
        "        self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)\n",
        "    )\n",
        "\n",
        "    return attn_outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "K9GKkupTk2p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.Linear tensordaki dense layerle oxsardir\n",
        "# forward ---> call methodu\n",
        "\n"
      ],
      "metadata": {
        "id": "6hjuopoqnFXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    embed_dim=config.hidden_size\n",
        "    num_heads=config.num_attention_heads\n",
        "    head_dim=embed_dim//num_heads\n",
        "    self.heads=nn.ModuleList(\n",
        "        [AttentionHead(embed_dim,head_dim) for _ in range(num_heads)]\n",
        "    )\n",
        "    self.output_linear=nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, hidden_state):\n",
        "    x=torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "    x=self.output_linear(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "8Hl9nMYrnmD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_outputs=MultiHeadAttention(config)(inputs_embeds)\n",
        "attn_outputs.size()"
      ],
      "metadata": {
        "id": "DBZ14JxhnmAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz import head_view\n",
        "from transformers import AutoModel\n",
        "\n",
        "model=AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n",
        "\n",
        "sentence_a='time flies like a arrow'\n",
        "sentence_b='fruit flies like a banana'\n",
        "\n",
        "viz_inputs= tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
        "attention=model(**viz_inputs).attentions\n",
        "sentence_b_start=(viz_inputs.token_type_ids==0).sum(dim=1)\n",
        "tokens=tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n",
        "\n",
        "head_view(attention, tokens,sentence_b_start, heads=[8])"
      ],
      "metadata": {
        "id": "vQSfm0rxnl-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.linear1=nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    self.linear2=nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.gelu=nn.GELU()\n",
        "    self.dropout=nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.linear1(x)\n",
        "    x=self.gelu(x)\n",
        "    x=self.linear2(x)\n",
        "    x=self.dropout(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "UuOMpkcNopZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_forward=FeedForward(config)\n",
        "ff_outputs=feed_forward(attn_outputs)\n",
        "ff_outputs.size()"
      ],
      "metadata": {
        "id": "ffny3ksxtXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layer_norm1=nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm2=nn.LayerNorm(config.hidden_size)\n",
        "    self.attention=MultiHeadAttention(config)\n",
        "    self.feed_forward=FeedForward(config)\n",
        "\n",
        "  def forward(self,x):\n",
        "    hidden_state=self.layer_norm1(x)\n",
        "    x=x+self.attention(hidden_state)\n",
        "    x=x+ self.feed_forward(self.layer_norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "6Z7sfoEvtXn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer=TransformerEncoderLayer(config)\n",
        "inputs_embeds.shape, encoder_layer(inputs_embeds).size()"
      ],
      "metadata": {
        "id": "VKXEoGy2tXlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Positional Embedding"
      ],
      "metadata": {
        "id": "ZXKYf63lw3fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.token_embeddings=nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "    self.position_embeddings=nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.layer_norm=nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
        "    self.dropout=nn.Dropout()\n",
        "\n",
        "  def forward(self,input_ids):\n",
        "    seq_length=input_ids.size(1)\n",
        "    position_ids=torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
        "    token_embeddings=self.token_embeddings(input_ids)\n",
        "    position_embeddings=self.position_embeddings(position_ids)\n",
        "    embeddings=token_embeddings+position_embeddings\n",
        "    embeddings=self.layer_norm(embeddings)\n",
        "    embeddings=self.dropout(embeddings)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "IIZy89bCwGW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#size(1) 1 cumlenin uzunlugu (0) batch gosterir\n",
        "#torch.long refers to torch.int64"
      ],
      "metadata": {
        "id": "XqLxGny4yNew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "YN2MVODEwGTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer=Embeddings(config)\n",
        "embedding_layer(inputs.input_ids).size()"
      ],
      "metadata": {
        "id": "9Hec_rcOwHM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.embeddings=Embeddings(config)\n",
        "    self.layers=nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.embeddings(x)\n",
        "    for layer in self.layers:\n",
        "      x=layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "13GwerDdwHJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=TransformerEncoder(config)\n",
        "encoder(inputs.input_ids).size()"
      ],
      "metadata": {
        "id": "xPM0xA2R0R6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding a Classification Head"
      ],
      "metadata": {
        "id": "VYv0KAxC0Zn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerForSequenceClassification(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.encoder=TransformerEncoder(config)\n",
        "    self.dropout=nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.classifier=nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.encoder(x)[:0,1]\n",
        "    x=self.dropout(x)\n",
        "    x=self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "xcAiDO5h1EYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_labels=3\n",
        "\n",
        "encoder_classifier=TransformerForSequenceClassification(config)\n",
        "encoder_classifier(inputs.input_ids).size()"
      ],
      "metadata": {
        "id": "mV1nKqe91EUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The decoder"
      ],
      "metadata": {
        "id": "bAMkpGKM1ESl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len=inputs.input_ids.size(-1)\n",
        "seq_len"
      ],
      "metadata": {
        "id": "93RHRh0t4Ltc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask=torch.tril(torch.ones(seq_len,seq_len)).unsqueeze(0)\n",
        "mask[0]"
      ],
      "metadata": {
        "id": "OgkSNV3N4Lp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores.masked_fill(mask==0,-float('inf'))"
      ],
      "metadata": {
        "id": "33OZOkPP4Lnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(querry, key, value, mask=None):\n",
        "  dim_k=key.size(-1)\n",
        "  scores=torch.bmm(query, key.transpose(1,2))/ sqrt(dim_k)\n",
        "  if mask is not None:\n",
        "    scores=scores.masked_fill(mask,float('-inf'))\n",
        "  weights=softmax(scores, dim=-1)\n",
        "  return torch.bmm(weights, value)\n"
      ],
      "metadata": {
        "id": "MCIB4qjR4LlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q39vPCwv6x2n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}