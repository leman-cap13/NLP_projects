{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/NLP_projects/blob/main/Web_scarping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W44OhK9MAQfy"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs"
      ],
      "metadata": {
        "id": "-40m7DgcBeL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load our first page"
      ],
      "metadata": {
        "id": "F6RfPu1oBonO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the webpage contenr\n",
        "r=requests.get('https://az.wikipedia.org/wiki/Ana_s%C9%99hif%C9%99')\n",
        "\n",
        "#convert to bs obj\n",
        "soup=bs(r.content)"
      ],
      "metadata": {
        "id": "rro4XHGUBq9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print our"
      ],
      "metadata": {
        "id": "lS-TAUKJCHkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup)"
      ],
      "metadata": {
        "id": "8zYGdBF1CVcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "65RruzJGCWxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Start to scarp"
      ],
      "metadata": {
        "id": "pYTgZWskCb6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find and findall"
      ],
      "metadata": {
        "id": "i8UuGTt4Cp5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_header=soup.find('h2')"
      ],
      "metadata": {
        "id": "_LHvrJXjCzAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_header #firts elemet with"
      ],
      "metadata": {
        "id": "Fwcvn90aC866"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header=soup.find_all('h2')"
      ],
      "metadata": {
        "id": "mSDfuFBXC_XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header"
      ],
      "metadata": {
        "id": "U3068VZeDF2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pass in a list of element for looking , h1 de meseln baxmaq isteyrik"
      ],
      "metadata": {
        "id": "422pC1tDDG2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_header=soup.find(['h1','h2'])"
      ],
      "metadata": {
        "id": "UGZQgIp8DV7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_header"
      ],
      "metadata": {
        "id": "EtHdVP-8Dae0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all(['h1','h2'])"
      ],
      "metadata": {
        "id": "izwH5yZ_Dcue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can pass in attribute to the find/find_all function\n",
        "paragraph=soup.find_all('p', attrs={'id':'paragraph-id'})\n",
        "paragraph#____????? atttributes ne uchun idi ?"
      ],
      "metadata": {
        "id": "4o5k1uT4D0kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can nest find/find_all calls\n",
        "body=soup.find('body')\n",
        "body"
      ],
      "metadata": {
        "id": "_ux2PrWPED-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "div=body.find('div')"
      ],
      "metadata": {
        "id": "VdghtyU0E2Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "div"
      ],
      "metadata": {
        "id": "Ef71rq_uE8AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header=div.find('h1')"
      ],
      "metadata": {
        "id": "55KkmKutE8hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header"
      ],
      "metadata": {
        "id": "73OlzJPRFFBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "cJrqznZbJtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stringi tapmaq uchun\n",
        "headers=soup.find_all('h2', string=re.compile('(H|h)eader'))"
      ],
      "metadata": {
        "id": "fUP3FZ7VJYLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers"
      ],
      "metadata": {
        "id": "pQqGsDtWJfAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### select (CSS selector)"
      ],
      "metadata": {
        "id": "bN8XLUpTKR9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contenet=soup.select('div p') # biz divin ichinde paraqrafi secmek uchun nested"
      ],
      "metadata": {
        "id": "v-t7NPvkKWhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contenet"
      ],
      "metadata": {
        "id": "Vp5bR62ZK07q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup.body.prettify()"
      ],
      "metadata": {
        "id": "RLz28Sg9K1sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs=soup.select('h2~p') # bele yazanda nested olmayanlari cixardir h2 p dalbadal"
      ],
      "metadata": {
        "id": "rW7I0eWqK7vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# <div>\n",
        "#   <h2>Başlıq</h2>\n",
        "#   <p>1-ci para</p>\n",
        "#   <span>Arada nəsə</span>\n",
        "#   <p>2-ci para</p>\n",
        "# </div>\n",
        "#-->soup.select('h2~p')\n",
        "# Nəticə: <p>1-ci para</p>, <p>2-ci para</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "osreooVJN-B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs"
      ],
      "metadata": {
        "id": "i62uXY9NLDG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bold_text=soup.select('paragraph-id b')"
      ],
      "metadata": {
        "id": "glBLflZANq4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bold_text"
      ],
      "metadata": {
        "id": "PCDdzwmKO_99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragarphs=soup.select('body >p')  # not nested child only direct child it uses\n",
        "print(paragarphs)\n",
        "\n",
        "for paragraph in paragarphs:\n",
        "  print(paragraph.select('i')  )"
      ],
      "metadata": {
        "id": "yIrC3pP-PA7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grap by element with specific property\n",
        "soup.select('[align=middle]')"
      ],
      "metadata": {
        "id": "xcjb-EbYQkxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Get different properties of the HTML"
      ],
      "metadata": {
        "id": "O9bt2txqQknO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "header=soup.find('h2')\n",
        "header.string #.string yalnız tag-in içində 1 dənə text varsa işləyir. Əgər içində başqa nested tag varsa, None qaytarır."
      ],
      "metadata": {
        "id": "bccQDxnRRAWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header"
      ],
      "metadata": {
        "id": "MKRUnr3qRATN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header.string"
      ],
      "metadata": {
        "id": "vnP-JHI1RAQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header.get_text(strip=True)"
      ],
      "metadata": {
        "id": "bFZJUg0ERMW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Əgər header tag-i birbaşa içində yalnız bir dənə text node saxlayırsa, header.string həmin mətnini qaytarır.\n",
        "\n",
        "Əgər header içində başqa tag varsa, header.string → None."
      ],
      "metadata": {
        "id": "pC-iGOz6Stnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tag içində yalnız mətn var, başqa heç nə yoxdur.\n",
        "\n",
        "Misal 2: header.string işləməyəcək (və None qaytaracaq)\n",
        "html\n",
        "Copy\n",
        "Edit\n",
        "<h2>Welcome <b>to</b> my site</h2>"
      ],
      "metadata": {
        "id": "tFSSH2mPS7Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "div=soup.find('div')\n",
        "print(div.prettify())\n",
        "print(div.string)"
      ],
      "metadata": {
        "id": "cD4U56jvScPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "header.get_text(strip=True) necə işləyir və niyə həmişə mətn qaytarır?\n",
        ".get_text() funksiyası:\n",
        "Tag-in içində nə qədər nested tag və mətn olsa belə, hamısını recursive şəkildə gəzir və mətn hissələri birləşdirir.\n",
        "\n",
        "strip=True istifadə edəndə əvvəl-axır boşluqları da silir."
      ],
      "metadata": {
        "id": "GIby2rZgTI9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(div.get_text(strip=True))"
      ],
      "metadata": {
        "id": "-aLYM2uaTBBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ".string sadəcə tag.contents siyahısının uzunluğu 1 və həmin element NavigableString olduqda işləyir.\n",
        "\n",
        ".get_text() isə tag.descendants ilə recursive gəzir, NavigableString tapanda toplayır və birləşd"
      ],
      "metadata": {
        "id": "o8FkMaxLTRkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get a specific property from an element\n",
        "link=soup.find('a')\n",
        "link"
      ],
      "metadata": {
        "id": "EDH_ftsdTA-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup.find_all('div', attrs={'class': True})\n"
      ],
      "metadata": {
        "id": "oevAoKmSTA8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "div.select('li#pt-sitesupport-2 a')[0]"
      ],
      "metadata": {
        "id": "NgqOriguU5xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link['href']"
      ],
      "metadata": {
        "id": "hBZmVeRDU5uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "select('li#pt-createaccount a') nə edir?\n",
        "\n",
        "li tag tapır, id=\"pt-createaccount olmalıdır.\n",
        "\n",
        "Onun içindəki bütün a tag-ları tapılır.\n",
        "\n",
        "✅ [0] → tapılan a tag-larından birincisini çıxarır.\n",
        "\n"
      ],
      "metadata": {
        "id": "_cvwqIdlXgCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs=soup.select('p#paragraph-id')"
      ],
      "metadata": {
        "id": "dzi-NaRYV0Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paragraphs[0]['id']"
      ],
      "metadata": {
        "id": "bKZTp9hMYUeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Path syntax\n",
        "soup.body.div.li"
      ],
      "metadata": {
        "id": "YKLIQDa9YUat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soup.body.div.prettify())"
      ],
      "metadata": {
        "id": "76Tc8VSKYiSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup.body.div.header"
      ],
      "metadata": {
        "id": "W4jrZovmY4ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#know the terms; parent sibling child\n",
        "soup.body.prettify()"
      ],
      "metadata": {
        "id": "ieDP5V38ZK2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P03D4Lc-Z6Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup.body.find('div').find_next_siblings()"
      ],
      "metadata": {
        "id": "axY-1tfRaY57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise"
      ],
      "metadata": {
        "id": "kygMY_ecas1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#load the wepsite"
      ],
      "metadata": {
        "id": "ADgBTDHMasyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the webpage contenr\n",
        "r=requests.get('https://keithgalli.github.io/web-scraping/webpage.html')\n",
        "\n",
        "#convert to bs obj\n",
        "webpage=bs(r.content)\n",
        "#print out our html\n",
        "print(webpage.prettify())"
      ],
      "metadata": {
        "id": "4kXopHKnaswS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Grab all of the social links from the webpage\n",
        "webpage.select('div', attrs={'class':'socials'})\n"
      ],
      "metadata": {
        "id": "H-vHpWxRcMXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNvF6ZxjcP63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_links=webpage.select('ul.socials a')"
      ],
      "metadata": {
        "id": "ABxiH49jcP4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_links"
      ],
      "metadata": {
        "id": "xwOIvi1AcP1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_links=webpage.select('ul.socials li')"
      ],
      "metadata": {
        "id": "L_8bWurNenSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_links"
      ],
      "metadata": {
        "id": "Co2GVjxTenPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ul – ul tag-ı (unordered list).\n",
        "\n",
        ".socials – class=\"socials\" olan ul tag-ı.\n",
        "\n",
        "Yəni ul tag-ı mütləq class=\"socials\" olmalıdır.\n",
        "\n",
        "li – həmin ul.socials tag-ının içindəki bütün li tag-larını tapacaq"
      ],
      "metadata": {
        "id": "-yl5ECLjfOm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for li in all_links:\n",
        "  link=li.a['href']\n",
        "  text=li.get_text(strip=True)\n",
        "  print(text,link)"
      ],
      "metadata": {
        "id": "OwKBrFZ_e0We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict={}\n",
        "for b, a in all_links:\n",
        "  my_dict[b.get_text(strip=True)]=a['href']\n",
        "my_dict"
      ],
      "metadata": {
        "id": "TWFm6NwTf0_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('a')"
      ],
      "metadata": {
        "id": "Kbn7xgfugIB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b_values=webpage.select('b')"
      ],
      "metadata": {
        "id": "pRCsgIuQgpIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in b_values:\n",
        "  print(b.get_text(strip=True))\n"
      ],
      "metadata": {
        "id": "0hnJXSo4hZ8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links=webpage.find('a') # sadece ilk tapdigini qaytarir find\n",
        "links"
      ],
      "metadata": {
        "id": "b_SBDNU1i9cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ulist=webpage.find('ul', attrs={'class':'socials'})\n",
        "links=ulist.find_all('a')\n",
        "links"
      ],
      "metadata": {
        "id": "zLu5wXZRi9ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links=webpage.select('li.social a')\n",
        "links"
      ],
      "metadata": {
        "id": "G_MgVMCwjUxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2"
      ],
      "metadata": {
        "id": "9qkx0DwOjsNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#scrape table"
      ],
      "metadata": {
        "id": "IeqoB5MXkXMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "ThVtGIHeoDJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('table.hockey-stats')[0]"
      ],
      "metadata": {
        "id": "Ga2Ym2oBlQt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('table.hockey-stats')[0].find('tr')"
      ],
      "metadata": {
        "id": "mFzJFvPelbJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('tbody')"
      ],
      "metadata": {
        "id": "VvcxowWZllWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('th.league')"
      ],
      "metadata": {
        "id": "w_mqAknIllS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('table.hockey-stats')[0].find('th')"
      ],
      "metadata": {
        "id": "1XwqXEf8mwKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('th.season')"
      ],
      "metadata": {
        "id": "EI19j76kmxnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_links=webpage.select('table.hockey-stats a')\n",
        "all_links"
      ],
      "metadata": {
        "id": "QvlxSUtOnNwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.find_all('hockey_stats', attrs={'class':'league'})"
      ],
      "metadata": {
        "id": "pLUPtl6xnZ7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table=webpage.select('table.hockey-stats')[0]"
      ],
      "metadata": {
        "id": "HPsg4E1Xnyq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns=table.find_all('th')\n",
        "columns=table.find('thead').find_all('th')\n",
        "columns"
      ],
      "metadata": {
        "id": "O2bTfGX5ph3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_name=[c.string for c in columns]\n",
        "columns_name"
      ],
      "metadata": {
        "id": "97xjKsVwqoSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows=table.find('tbody').find_all('tr')\n",
        "rows"
      ],
      "metadata": {
        "id": "amGfqEDZqxiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=[]\n",
        "for tr in rows:\n",
        "  td=tr.find_all('td')\n",
        "  row=[tr.text.strip() for tr in td]\n",
        "  l.append(row)\n",
        "print(l)"
      ],
      "metadata": {
        "id": "2xVrBbEFouOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(l, columns=columns_name)"
      ],
      "metadata": {
        "id": "g-O1Oh-6phZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3"
      ],
      "metadata": {
        "id": "akLVkSQvrUI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#grab all fun facts use word 'is"
      ],
      "metadata": {
        "id": "CdxGUJ5Nwa1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fun_facts=webpage.select('ul.fun-facts li')"
      ],
      "metadata": {
        "id": "EpHzk445tQR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fun_facts"
      ],
      "metadata": {
        "id": "q4wqxY71tsjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in webpage.select('ul.fun-facts li'):\n",
        "  print(i.get_text(strip=True))"
      ],
      "metadata": {
        "id": "baa4SMp6tuab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fun_facts_with_is=[i.get_text('',strip=True) for i in webpage.select('ul.fun-facts li') if 'is' in i.get_text('',strip=True)]"
      ],
      "metadata": {
        "id": "trV1xpSat9Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fun_facts_with_is"
      ],
      "metadata": {
        "id": "5FnCiKuXuc_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "facts=webpage.select('ul.fun-facts li')\n",
        "facts_with_is=[fact.find(string=re.compile('is')) for fact in facts]\n",
        "facts_with_is=[fact.find_parent().get_text() for fact in facts_with_is if fact]\n",
        "facts_with_is\n"
      ],
      "metadata": {
        "id": "7o6jEHAKud4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "facts=webpage.select('ul.fun-facts li')\n",
        "facts_with_is=[fact.find(string=re.compile('is')) for fact in facts]\n",
        "facts_with_is=[fact for fact in facts_with_is if fact]\n",
        "facts_with_is"
      ],
      "metadata": {
        "id": "jQFtsSx_wRZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "facts=webpage.select('ul.fun-facts li')\n",
        "facts_with_is=[fact.find(string=re.compile('is')) for fact in facts]\n",
        "facts_with_is=[fact.find_parent() for fact in facts_with_is if fact]\n",
        "facts_with_is"
      ],
      "metadata": {
        "id": "_K4UuY36xjv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4"
      ],
      "metadata": {
        "id": "vOryAuNtx6Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#download an image"
      ],
      "metadata": {
        "id": "KqIFvvnjxvvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image=webpage.select('div.row')"
      ],
      "metadata": {
        "id": "X2Hrv0H1x8PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "uG4JayDnx8Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('div', attrs={'class':'row'})"
      ],
      "metadata": {
        "id": "ZpBjO67gyVkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webpage.select('div', attrs={'class':'src'})"
      ],
      "metadata": {
        "id": "XNGXi2WMyb_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "hxna3o7a0Hzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests # pip install requests\n",
        "from bs4 import BeautifulSoup as bs # pip install beautifulsoup4\n",
        "\n",
        "# Load the webpage content\n",
        "url = \"https://keithgalli.github.io/web-scraping/\"\n",
        "r = requests.get(url+\"webpage.html\")"
      ],
      "metadata": {
        "id": "SwY5a2HB02l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image=webpage.select('div.row div.column img')"
      ],
      "metadata": {
        "id": "P_D4h6ht1r4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url=image[0]['src']"
      ],
      "metadata": {
        "id": "r6_M3CS61sQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url"
      ],
      "metadata": {
        "id": "P-QRYFcD10R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_url=url+image_url"
      ],
      "metadata": {
        "id": "P8iA6lBT13A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_data = requests.get(full_url).content\n",
        "with open('lake_como.jpg', 'wb') as handler:\n",
        "    handler.write(img_data)"
      ],
      "metadata": {
        "id": "VWhYtd6Q2BCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvmhmGqG2jIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N-K6JJXPl24L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "E88mpGopl3PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/all_azerbaijani_news_dataset (6).csv\")\n",
        "\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "V8nANcv-l3L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#m"
      ],
      "metadata": {
        "id": "oskWwHQCm7oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('site',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "DiCQ5u_WmwUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "OaTaQij4nBbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets huggingface_hub pandas"
      ],
      "metadata": {
        "id": "nRFGqaj7nCrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "UKV2N8oKn7El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "dff = pd.read_parquet(\"hf://datasets/lilas12/laman-az-news-lent/data/train-00000-of-00001-efeb537f6586a389.parquet\")"
      ],
      "metadata": {
        "id": "evAOaxaWoGPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff"
      ],
      "metadata": {
        "id": "sBqIk-jZoKTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined = pd.concat([df, dff], ignore_index=True)\n",
        "print(df_combined.shape)  # yoxlamaq üçün\n"
      ],
      "metadata": {
        "id": "R_ZyMCFGoOD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined"
      ],
      "metadata": {
        "id": "bqvUKtiIoc-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Yeni topladığın df: {len(df)}\")\n",
        "print(f\"HF-dən yüklədiyin dff: {len(dff)}\")\n"
      ],
      "metadata": {
        "id": "EQeyOfE8qFVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Pandas DataFrame-i Hugging Face Dataset-ə çevir\n",
        "dataset_combined = Dataset.from_pandas(df_combined)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ab6EL19Rq3ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_combined.push_to_hub(\n",
        "    \"lilas12/laman-az-news-lent\",  # eyni dataset adı\n",
        "    split=\"train\"\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "p0eclQkhq3Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xADFo1qeqsVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnfJQGjVqsI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8NDVVOmqqsGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#scrape"
      ],
      "metadata": {
        "id": "qXTBvwTmqsDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import pandas as pd # CSV-yə çevirmək üçün\n",
        "\n",
        "all_articles_data = []\n",
        "scraped_urls = set() # Duplikatları yoxlamaq üçün artıq scrape edilmiş URL-ləri saxlayır\n",
        "\n",
        "# --- KONFİQURASİYA ---\n",
        "# Başlanğıc URL-lər\n",
        "# Kulis.az-ın əsas səhifəsi və ya məqalələr səhifəsi\n",
        "start_url = \"https://kulis.az/\"\n",
        "max_pages_per_category = 200 # Hər kateqoriyadan maksimum neçə səhifə scrape ediləcək. Çox böyük rəqəm serverə zərər verə bilər!\n",
        "\n",
        "# CSV faylının adı\n",
        "csv_output_file = 'kulis_all_articles.csv'\n",
        "json_output_file = 'kulis_all_articles.json'\n",
        "\n",
        "# --- YARDIMÇI FUNKSİYALAR ---\n",
        "\n",
        "def get_full_url(base, relative_url):\n",
        "    \"\"\"Nisbi URL-i tam URL-ə çevirir.\"\"\"\n",
        "    if relative_url.startswith('http'):\n",
        "        return relative_url\n",
        "    return requests.compat.urljoin(base, relative_url)\n",
        "\n",
        "def scrape_article_content(article_url):\n",
        "    \"\"\"Verilmiş məqalə URL-dən məzmunu, müəllifi və tarixi çəkir.\"\"\"\n",
        "    article_data = {\n",
        "        \"title\": \"N/A\",\n",
        "        \"url\": article_url,\n",
        "        \"author\": \"N/A\",\n",
        "        \"publish_date\": \"N/A\",\n",
        "        \"content\": \"N/A\"\n",
        "    }\n",
        "\n",
        "    if article_url in scraped_urls:\n",
        "        # print(f\"    URL artıq scrape edilib, keçilir: {article_url}\")\n",
        "        return None # Artıq scrape edilmiş URL-ləri təkrar scrape etmə\n",
        "\n",
        "    try:\n",
        "        article_response = requests.get(article_url, timeout=15)\n",
        "        article_response.raise_for_status()\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "        # Məqalə başlığını tapmaq üçün əsas HTML elementi (şəkillərdəki müşahidəyə əsasən)\n",
        "        # Bəzi hallarda məqalənin öz səhifəsində başqa bir yerdə də ola bilər (məsələn, <h1>)\n",
        "        title_tag_on_page = article_soup.find('h1', class_='page-title') # Ümumi başlığın sinfi\n",
        "        if title_tag_on_page:\n",
        "            article_data['title'] = title_tag_on_page.text.strip()\n",
        "        elif article_soup.find('meta', property='og:title'): # SEO meta taglardan başlığı almaq\n",
        "            article_data['title'] = article_soup.find('meta', property='og:title')['content'].strip()\n",
        "\n",
        "        # Məqalə mətnini tapın ('text-zone' və ya birbaşa mətn)\n",
        "        article_content_div = article_soup.find('div', class_='text-zone')\n",
        "        if article_content_div:\n",
        "            paragraphs = article_content_div.find_all('p')\n",
        "            if paragraphs:\n",
        "                article_data['content'] = '\\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
        "            else:\n",
        "                direct_text = article_content_div.get_text(separator='\\n', strip=True)\n",
        "                if direct_text:\n",
        "                    article_data['content'] = direct_text\n",
        "\n",
        "        # Müəllif və nəşr tarixini tapın (Kulis.az-da adətən)\n",
        "        author_tag = article_soup.find('div', class_='author-name')\n",
        "        if author_tag:\n",
        "            article_data['author'] = author_tag.text.strip()\n",
        "\n",
        "        date_tag = article_soup.find('div', class_='author-time')\n",
        "        if date_tag:\n",
        "            article_data['publish_date'] = date_tag.text.strip()\n",
        "\n",
        "        scraped_urls.add(article_url) # URL-i artıq scrape edilmişlər siyahısına əlavə et\n",
        "        return article_data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"    Xəta: Məqalə ({article_url}) yüklənərkən şəbəkə xətası: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Xəta: Məqalə ({article_url}) işlənərkən gözlənilməyən xəta: {e}\")\n",
        "    return None\n",
        "\n",
        "# --- ƏSAS SCRAPING MƏNTİQİ ---\n",
        "\n",
        "# 1. Bütün kateqoriya URL-lərini tapmaq\n",
        "print(\"Kateqoriya URL-ləri tapılır...\")\n",
        "category_urls = set()\n",
        "try:\n",
        "    initial_response = requests.get(start_url, timeout=15)\n",
        "    initial_response.raise_for_status()\n",
        "    initial_soup = BeautifulSoup(initial_response.text, 'html.parser')\n",
        "\n",
        "    # Kateqoriya linkləri adətən naviqasiya menyusunda olur.\n",
        "    # Kulis.az-da bu linklər adətən header-dəki nav bar-dadır.\n",
        "    # Siz brauzerdə \"İncele\" edərək bu linklərin yerləşdiyi div/ul elementlərini tapmalısınız.\n",
        "    # Nümunə üçün, 'top-menu' kimi bir class axtara bilərik, lakin dəqiqliyi yoxlayın!\n",
        "\n",
        "    # Kulis.az üçün naviqasiya menu id=\"primary-menu\" olan UL içindədir\n",
        "    nav_menu = initial_soup.find('ul', id='primary-menu')\n",
        "    if nav_menu:\n",
        "        for a_tag in nav_menu.find_all('a', href=True):\n",
        "            cat_url = get_full_url(start_url, a_tag['href'])\n",
        "            # Yalnız 'articles', 'xeber', 'culture' və s. ilə başlayan linkləri götür\n",
        "            if '/articles/' in cat_url or '/xeber/' in cat_url or '/culture/' in cat_url or '/literature/' in cat_url:\n",
        "                if '?' not in cat_url: # Əgər səhifə parametri yoxdursa, əlavə edək\n",
        "                    cat_url += '?page=1'\n",
        "                elif 'page=' not in cat_url: # Əgər digər parametrlər varsa, page-i əlavə edək\n",
        "                    cat_url += '&page=1'\n",
        "\n",
        "                # Səhifə nömrəsini çıxarmaq üçün formatı hazırlayın\n",
        "                # Məsələn, https://kulis.az/kino?page={}\n",
        "                cat_url_template = re.sub(r'page=\\d+', 'page={}', cat_url)\n",
        "                if '{}' in cat_url_template: # Yalnız səhifə nömrəsinə görə dəyişə bilən URL-ləri əlavə edin\n",
        "                    category_urls.add(cat_url_template)\n",
        "    else:\n",
        "        print(\"Xəbərdarlıq: Əsas naviqasiya menyusu tapılmadı. Yalnız əsas səhifə məqalələri scrape ediləcək.\")\n",
        "        # Əgər naviqasiya tapılmasa, sadəcə Kino kateqoriyası ilə başlayaq (əvvəlki kimi)\n",
        "        category_urls.add(\"https://kulis.az/kino?page={}\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Xəta: Əsas səhifə ({start_url}) yüklənərkən şəbəkə xətası: {e}\")\n",
        "    print(\"Kateqoriya linkləri tapılmadı. Proses dayandırılır.\")\n",
        "    # Əgər kateqoriya tapılmasa, birbaşa Kino kateqoriyasından başlayaq\n",
        "    category_urls.add(\"https://kulis.az/kino?page={}\")\n",
        "\n",
        "# Əgər heç bir kateqoriya tapılmasa, əsas articles səhifəsini əlavə et\n",
        "if not category_urls:\n",
        "    category_urls.add(\"https://kulis.az/articles?page={}\")\n",
        "\n",
        "\n",
        "print(f\"Tapılan unikal kateqoriya URL şablonları ({len(category_urls)}):\")\n",
        "for url_template in category_urls:\n",
        "    print(f\"- {url_template}\")\n",
        "time.sleep(2)\n",
        "\n",
        "\n",
        "# 2. Hər kateqoriyadakı bütün səhifələri scrape etmək\n",
        "for category_template_url in category_urls:\n",
        "    print(f\"\\nKateqoriya: {category_template_url.split('?')[0].replace('https://kulis.az/', '')} scrape edilir.\")\n",
        "    current_page_num_in_category = 1\n",
        "\n",
        "    while current_page_num_in_category <= max_pages_per_category:\n",
        "        current_category_page_url = category_template_url.format(current_page_num_in_category)\n",
        "        print(f\"  Scraping page {current_page_num_in_category}: {current_category_page_url}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(current_category_page_url, timeout=15)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            main_content_div = soup.find('div', id='content', class_='block')\n",
        "            if not main_content_div:\n",
        "                print(f\"    Səhifə {current_category_page_url}-də əsas məzmun bloku tapılmadı. Növbəti kateqoriyaya keçilir.\")\n",
        "                break # Bu səhifədə məzmun yoxdursa, növbəti səhifələrdə də olmayacaq\n",
        "\n",
        "            article_links_on_page = main_content_div.find_all('a', class_='item')\n",
        "\n",
        "            if not article_links_on_page:\n",
        "                print(f\"    Səhifə {current_category_page_url}-də məqalə linkləri tapılmadı. Bu kateqoriya bitdi.\")\n",
        "                break # Bu səhifədə məqalə yoxdursa, deməli son səhifəyə çatmışıq\n",
        "\n",
        "            for link_tag in article_links_on_page:\n",
        "                full_article_url = get_full_url(current_category_page_url, link_tag.get('href'))\n",
        "\n",
        "                article_data = scrape_article_content(full_article_url)\n",
        "                if article_data:\n",
        "                    all_articles_data.append(article_data)\n",
        "                    # print(f\"      Əlavə edildi: {article_data['title']}\") # Debug üçün aktivləşdirə bilərsiniz\n",
        "                time.sleep(0.5) # Hər məqalə arasında qısa gecikmə\n",
        "\n",
        "            current_page_num_in_category += 1\n",
        "            time.sleep(3) # Hər kateqoriya səhifəsi arasında daha uzun gecikmə\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"  Xəta: Kateqoriya səhifəsi ({current_category_page_url}) yüklənərkən şəbəkə xətası: {e}. Növbəti kateqoriyaya keçilir.\")\n",
        "            break # Xəta olarsa, bu kateqoriyadan çıx\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Xəta: Kateqoriya səhifəsi ({current_category_page_url}) işlənərkən gözlənilməyən xəta: {e}. Növbəti kateqoriyaya keçilir.\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nToplam {len(all_articles_data)} unikal məqalə toplandı.\")\n",
        "\n",
        "# Məlumatları JSON faylına yazmaq\n",
        "with open(json_output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_articles_data, f, ensure_ascii=False, indent=4)\n",
        "print(f\"Məlumatlar '{json_output_file}' faylına yazıldı.\")\n",
        "\n",
        "# JSON-dan CSV-yə çevirmək\n",
        "if all_articles_data:\n",
        "    try:\n",
        "        df = pd.DataFrame(all_articles_data)\n",
        "        df.to_csv(csv_output_file, index=False, encoding='utf-8-sig')\n",
        "        print(f\"Məlumatlar '{csv_output_file}' faylına da yazıldı.\")\n",
        "    except Exception as e:\n",
        "        print(f\"CSV faylına çevrilərkən xəta baş verdi: {e}\")\n",
        "else:\n",
        "    print(\"Toplanmış məqalə yoxdur, CSV faylı yaradılmadı.\")"
      ],
      "metadata": {
        "id": "2azGLdy2quIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infodf = pd.read_csv('kulis_all_articles.csv')"
      ],
      "metadata": {
        "id": "Qzqz74RlquFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infodf"
      ],
      "metadata": {
        "id": "TeGCyxCT-X4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_article_content(article_url):\n",
        "    \"\"\"Verilmiş məqalə URL-dən məzmunu, müəllifi və tarixi çəkir.\"\"\"\n",
        "    article_data = {\n",
        "        \"title\": \"N/A\",\n",
        "        \"url\": article_url,\n",
        "        \"author\": \"N/A\",\n",
        "        \"publish_date\": \"N/A\",\n",
        "        \"content\": \"N/A\"\n",
        "    }\n",
        "\n",
        "    if article_url in scraped_urls:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        article_response = requests.get(article_url, timeout=15)\n",
        "        article_response.raise_for_status()\n",
        "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
        "\n",
        "        # Title\n",
        "        title_tag_on_page = article_soup.find('h1', class_='page-title')\n",
        "        if title_tag_on_page:\n",
        "            article_data['title'] = title_tag_on_page.get_text(strip=True)\n",
        "        elif article_soup.find('meta', property='og:title'):\n",
        "            article_data['title'] = article_soup.find('meta', property='og:title')['content'].strip()\n",
        "\n",
        "        # Content\n",
        "        article_content_div = article_soup.find('div', class_='news__body')\n",
        "        if article_content_div:\n",
        "            paragraphs = article_content_div.find_all('p')\n",
        "            if paragraphs:\n",
        "                article_data['content'] = '\\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
        "            else:\n",
        "                article_data['content'] = article_content_div.get_text(separator='\\n', strip=True)\n",
        "\n",
        "        # Author\n",
        "        author_tag = article_soup.find('div', class_='author-name')\n",
        "        if author_tag:\n",
        "            article_data['author'] = author_tag.get_text(strip=True)\n",
        "\n",
        "        # Publish Date\n",
        "        date_tag = article_soup.find('div', class_='author-time')\n",
        "        if date_tag:\n",
        "            article_data['publish_date'] = date_tag.get_text(strip=True)\n",
        "\n",
        "        scraped_urls.add(article_url)\n",
        "        return article_data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"    Xəta: Məqalə ({article_url}) yüklənərkən şəbəkə xətası: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Xəta: Məqalə ({article_url}) işlənərkən gözlənilməyən xəta: {e}\")\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "PlXIf4LlMp6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_url = \"https://kulis.az/news/48757\"\n",
        "test_article = scrape_article_content(test_url)\n",
        "print(json.dumps(test_article, ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "id": "xm9NM1aCMp4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_pB53xeMp1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_2iImT3MpzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3s_DP8FCYX61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tIZ5NzVYX4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "_DVNx-1sYXxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf pandas datasets huggingface_hub\n"
      ],
      "metadata": {
        "id": "cLKojEMfZHcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Fayl yolunu dəyiş\n",
        "pdf_path = \"/content/combined_books.pdf\"\n",
        "pdf = fitz.open(pdf_path)\n",
        "\n",
        "text = \"\"\n",
        "for page in pdf:\n",
        "    text += page.get_text()\n",
        "\n",
        "pdf.close()\n",
        "\n",
        "print(text[:1000])  # ilk 1000 simvolu yoxlamaq üçün\n"
      ],
      "metadata": {
        "id": "94RYp_MHZHZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# İstəyirsənsə, mətni hissələrə bölə bilərsən (RAG və ya uzun fayllar üçün)\n",
        "# Hazırda sadəcə 1 sütunlu CSV saxlayırıq:\n",
        "df = pd.DataFrame({\"text\": [text]})\n",
        "\n",
        "df.to_csv(\"my_pdf_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "M-NIyK0WZT57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "__iGYP6dZuZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()  # burda token daxil edəcəksən\n"
      ],
      "metadata": {
        "id": "rgrV5tmRZZht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# CSV-dən dataset yarat\n",
        "dataset = Dataset.from_csv(\"my_pdf_dataset.csv\")\n",
        "\n",
        "# Öz istifadəçi adın və istədiyin dataset adını qoy\n",
        "dataset.push_to_hub(\"your_username/my_pdf_dataset\")\n"
      ],
      "metadata": {
        "id": "-g0qWLaMZT3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}