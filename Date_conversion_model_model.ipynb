{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPy4V7r3JlMs6tejxLdjGUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/Date_conversion_model_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-07tNyo-yo9"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# cannot use strftime()'s %B format since it depends on the locale\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "\n",
        "\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "\n",
        "\n",
        "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
        "    y = [dt.isoformat() for dt in dates]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "n_dates = 100_000\n",
        "x_example, y_example = random_dates(n_dates)\n",
        "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
        "print(\"-\" * 50)\n",
        "for idx in range(3):\n",
        "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_example[:10]"
      ],
      "metadata": {
        "id": "1AXVpC1aMrVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_example[:10]"
      ],
      "metadata": {
        "id": "NiIw_8g-Mgqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ittN9fv9DcBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "xOAZCPx5JplW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make input and target tokenization\n",
        "max_len=30\n",
        "embed_size=128\n",
        "input_vec_layer=tf.keras.layers.TextVectorization( output_sequence_length=max_len,split='character', standardize='lower')\n",
        "input_vec_layer.adapt(x_example)\n",
        "\n",
        "target_vec_layer=tf.keras.layers.TextVectorization(output_sequence_length=max_len,split='character', standardize='lower')\n",
        "target_vec_layer.adapt([f\"startofseq {s} endofseq\" for s in y_example])"
      ],
      "metadata": {
        "id": "zsbY1gWrBbiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(input_vec_layer.get_vocabulary())\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "UjCSzLI1MjV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_vocab_size=len(target_vec_layer.get_vocabulary())\n",
        "target_vocab_size"
      ],
      "metadata": {
        "id": "8LtKb45LPwrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_vec_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "k78qdVsyL0E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_vec_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "esOVbH5XL30x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input layer\n",
        "tf.random.set_seed(42)\n",
        "encoder_inputs=tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs=tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "XotG5MMnN-wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs"
      ],
      "metadata": {
        "id": "6LMyfIpqNSri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs.shape"
      ],
      "metadata": {
        "id": "KozbA96a5PIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs.shape"
      ],
      "metadata": {
        "id": "gI1uh8Gc5Sys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization qatlarını modelə daxil et\n",
        "encoder_input=input_vec_layer(encoder_inputs)\n",
        "decoder_input=target_vec_layer(decoder_inputs)\n"
      ],
      "metadata": {
        "id": "DRboe7ukaOLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input"
      ],
      "metadata": {
        "id": "HVNU3jtoNald"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input.shape"
      ],
      "metadata": {
        "id": "w-Dn9YcH5Vye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input.shape"
      ],
      "metadata": {
        "id": "2DozYOQj5Y2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#növbəti addım: Embedding qatları\n",
        "encoder_embedding_layer=tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True)\n",
        "decoder_embedding_layer=tf.keras.layers.Embedding(input_dim=target_vocab_size, output_dim=128,mask_zero=True)"
      ],
      "metadata": {
        "id": "PZqiHRv0brv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embedding_layer"
      ],
      "metadata": {
        "id": "L2izECx0Pr_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder_embedding_layer.weights[0].shape"
      ],
      "metadata": {
        "id": "fGzo-7aH5baQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoder_embedding_layer.weights[0].shape"
      ],
      "metadata": {
        "id": "KTY8MrLi5rHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  fit embedding layers\n",
        "encoder_embedding=encoder_embedding_layer(encoder_input)\n",
        "decoder_embedding=decoder_embedding_layer(decoder_input)"
      ],
      "metadata": {
        "id": "rzgU8gmrcKei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embedding"
      ],
      "metadata": {
        "id": "Y9gnuTDwQIC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_embedding.shape"
      ],
      "metadata": {
        "id": "oqjX39JG5uUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_embedding.shape"
      ],
      "metadata": {
        "id": "7FUCoxYI5xIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder mask\n",
        "# encoder_mask = encoder_embedding_layer.compute_mask(encoder_input)\n",
        "# decoder_mask = decoder_embedding_layer.compute_mask(decoder_input)"
      ],
      "metadata": {
        "id": "CcShHhrKoftu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder LSTM LAYER\n",
        "encoder_lstm=tf.keras.layers.LSTM(512, return_state=True,return_sequences=True)\n",
        "encoder_outputs, *encoder_states = encoder_lstm(encoder_embedding)\n",
        "# mask=encoder_mask\n",
        "\n",
        "\n",
        "#Decoder LSTM layer\n",
        "decoder_lstm=tf.keras.layers.LSTM(512,return_sequences=True)\n",
        "decoder_outputs= decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "#mask=decoder_mask"
      ],
      "metadata": {
        "id": "r6xJ0J16c4op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiHeadAttention\n",
        "embed_dim = 512\n",
        "num_heads = 2\n",
        "mha_outputs = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(query=decoder_outputs,\n",
        "                                                                                          value=encoder_outputs,\n",
        "                                                                                          key=encoder_outputs)\n",
        "concat_outputs=tf.keras.layers.Concatenate()([decoder_outputs,mha_outputs])"
      ],
      "metadata": {
        "id": "r4LSt6deojWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_outputs.shape"
      ],
      "metadata": {
        "id": "h7oMODL-51GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_outputs.shape"
      ],
      "metadata": {
        "id": "lfwluAVi6A87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(encoder_outputs))\n",
        "print(type(decoder_outputs))"
      ],
      "metadata": {
        "id": "yANDEpkcVggd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#attention layer\n",
        "# attention_layer=tf.keras.layers.Attention()\n",
        "# attention_outputs=attention_layer([decoder_outputs, encoder_outputs], mask=[decoder_mask, encoder_mask])"
      ],
      "metadata": {
        "id": "FF_tszdnbdIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output layer\n",
        "output_layer=tf.keras.layers.Dense(target_vocab_size,activation=\"softmax\")\n",
        "Y_proba=output_layer(concat_outputs)"
      ],
      "metadata": {
        "id": "8Ori2anteRyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train test split\n",
        "X_train=tf.constant(x_example[:60_000])\n",
        "X_valid=tf.constant(x_example[60_000:])\n",
        "X_train_dec=tf.constant([f\"startofseq {s}\" for s in y_example[:60_000]])\n",
        "X_valid_dec=tf.constant([f\"startofseq {s}\" for s in y_example[60_000:]])\n",
        "Y_train=target_vec_layer([f\"{s} endofseq\" for s in y_example[:60_000]])\n",
        "Y_valid=target_vec_layer([f\"{s} endofseq\" for s in y_example[60_000:]])"
      ],
      "metadata": {
        "id": "slzKXl839lpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "CMK7MqdTBVSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_dec.shape"
      ],
      "metadata": {
        "id": "xkpYDfG_BaTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "id": "hmzY4xPuBcp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make model\n",
        "model=tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "c_YQzi-vfgXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit((X_train, X_train_dec), Y_train, epochs=3, validation_data=((X_valid,X_valid_dec),Y_valid))"
      ],
      "metadata": {
        "id": "1g9Y2Igz_hdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_date(x_example):\n",
        "    translation = ''\n",
        "    for word_idx in range(max_len):\n",
        "        X = tf.constant([x_example])\n",
        "        dec_input = tf.constant([f\"startofseq {translation}\".strip()])\n",
        "        y_proba = model.predict([X, dec_input], verbose=0)[0, word_idx]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = target_vec_layer.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == 'endofseq':\n",
        "            break\n",
        "\n",
        "        translation += predicted_word\n",
        "    return translation\n"
      ],
      "metadata": {
        "id": "C7YfPPujTPDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len"
      ],
      "metadata": {
        "id": "hAKTGm8FY6Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_date('September 20, 7075')"
      ],
      "metadata": {
        "id": "MmhDGh0bF3pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "9PqBcy1XH7oR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}