{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsMP3WMvLogy06MK70gm32",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/RAG_COVID_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model"
      ],
      "metadata": {
        "id": "J6kgpJHUVWXH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goC6M3yDNhfZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "zFd89cePTD-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d xhlulu/covidqa\n"
      ],
      "metadata": {
        "id": "nyXoIgqeXwc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o covidqa.zip -d covidqa_original\n",
        "!ls covidqa_original\n"
      ],
      "metadata": {
        "id": "P5e6C7qaX4tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir('/content/covidqa_original'))\n"
      ],
      "metadata": {
        "id": "7SqP6q4UXZq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/covidqa_original/community.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "G5sen2v5TkjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "UMYUlyC5Uxtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['question_id','answer_id','url', 'source', 'answer_type',  'wrong_answer_type',], inplace=True)"
      ],
      "metadata": {
        "id": "YRDdNrWsTkfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "QqJTsLhaVF9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns  #Index(['title', 'question', 'answer', 'wrong_answer'], dtype='object')"
      ],
      "metadata": {
        "id": "zhRFi4GmVGfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the Retrieval Corpus\n",
        "#Create a list of documents/passages to index for retrieva\n",
        "\n",
        "# Combine 'title' and 'answer' as one passage\n",
        "df['passage'] = df['title'].fillna('') + '. ' + df['answer'].fillna('')\n",
        "\n",
        "# Check some passages\n",
        "print(df['passage'].head())\n"
      ],
      "metadata": {
        "id": "6iHR-ACyVU5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embed Passages with SentenceTransformer and Build FAISS Index"
      ],
      "metadata": {
        "id": "Om8TYfp3ZoCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "id": "pKzi4WkSZq_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from faiss import IndexFlatL2\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "zpr8NkyOZq8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed the passages (list of strings)\n",
        "passages = df['passage'].tolist()\n",
        "corpus_embeddings = embedder.encode(passages,\n",
        "                                    convert_to_numpy=True,\n",
        "                                    show_progress_bar=True)"
      ],
      "metadata": {
        "id": "i2zuUwiHaGKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "embedder.encode()\n",
        "This is a method from the SentenceTransformer model that takes a list of texts (in my case, the passages) and converts each text into a fixed-length numeric vector, called an embedding.\n",
        "\n",
        "passages\n",
        "This is the list of your combined 'title' + 'answer' strings from your dataframe. Each passage is a text sample you want to represent as a vector.\n",
        "\n",
        "convert_to_numpy=True\n",
        "By default, encode() returns a PyTorch tensor or a list of lists. Setting convert_to_numpy=True means the output will be a NumPy array, which is convenient and compatible with libraries like FAISS.\n",
        "\n",
        "show_progress_bar=True\n",
        "This displays a progress bar during embedding, so you can visually see how fast or slow it is, especially useful when embedding many passages."
      ],
      "metadata": {
        "id": "2k2KcEgKaciR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "why convert_to_numpy=True?\n",
        "Because:\n",
        "\n",
        "FAISS (Facebook‚Äôs fast similarity search library) works with NumPy arrays (or raw float32 arrays).\n",
        "\n",
        "PyTorch tensors or lists aren‚Äôt accepted directly by FAISS without conversion.\n",
        "\n",
        "NumPy is lightweight and fast for vector ops like cosine similarity, dot product, etc."
      ],
      "metadata": {
        "id": "4BtCrs1layR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is FAISS?\n",
        "FAISS stands for:\n",
        "\n",
        "Facebook AI Similarity Search\n",
        "\n",
        "It‚Äôs an open-source library for fast similarity search over dense vectors (i.e., embeddings).\n",
        "\n",
        "üîç Why do we need it?\n",
        "In Retrieval-Augmented Generation (RAG), we:\n",
        "\n",
        "Embed all documents (your passages) into vectors using a model like Sentence-BERT\n",
        "\n",
        "Embed a question into a vector (the user query)\n",
        "\n",
        "Search for the most similar document vectors to the query vector\n",
        "\n",
        "üîó This is where FAISS comes in ‚Äî it's the search engine for vector similarity."
      ],
      "metadata": {
        "id": "JlEN3wARbUDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings.shape"
      ],
      "metadata": {
        "id": "rFJuaCB3b0Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings"
      ],
      "metadata": {
        "id": "dDkoWuv3b2yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_embeddings.shape[1]"
      ],
      "metadata": {
        "id": "4HTomWfSb_EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "dimension = corpus_embeddings.shape[1]  # embedding size, e.g., 384\n",
        "index = faiss.IndexFlatIP(dimension)  # Inner Product similarity (cosine if normalized)\n",
        "\n",
        "# Normalize embeddings if using IP similarity for cosine similarity\n",
        "faiss.normalize_L2(corpus_embeddings)\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "print(f\"FAISS index contains {index.ntotal} passages\")\n"
      ],
      "metadata": {
        "id": "qROugJjIaffm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "corpus_embeddings is your NumPy array of shape (N, D) where:\n",
        "\n",
        "N = number of passages (e.g., 1000)\n",
        "\n",
        "D = embedding dimension (e.g., 384 for MiniLM)\n",
        "\n"
      ],
      "metadata": {
        "id": "oIm_B100cX-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "index = faiss.IndexFlatIP(dimension)\n",
        "This creates a flat (brute-force) FAISS index that uses:\n",
        "\n",
        "IP = Inner Product similarity (also known as dot product)\n",
        "\n",
        "IP can approximate cosine similarity if your vectors are normalized (see next line).\n",
        "\n",
        "\"Flat\" means no approximation ‚Äî it compares the query to every document embedding."
      ],
      "metadata": {
        "id": "GULFRmXqcfch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "faiss.normalize_L2(corpus_embeddings)\n",
        "This transforms each embedding into a unit vector (vector length = 1).\n",
        "\n",
        "Why?\n",
        "\n",
        "When vectors are normalized, inner product (dot product) becomes cosine similarity.\n",
        "\n",
        "This is a standard trick to use cosine similarity with FAISS."
      ],
      "metadata": {
        "id": "UHCVZzDScsW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform Query Retrieval"
      ],
      "metadata": {
        "id": "z_PgOQanafcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means:\n",
        "\n",
        "You enter a question.\n",
        "\n",
        "We embed that question using the same SentenceTransformer.\n",
        "\n",
        "We normalize the query vector (just like we did with the corpus).\n",
        "\n",
        "We pass it to FAISS to get top-k most similar passages."
      ],
      "metadata": {
        "id": "JNrWrLEldQZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define your query\n",
        "query = \"How does COVID-19 spread?\"\n",
        "\n",
        "# Step 2: Embed the query (same model as corpus)\n",
        "query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "\n",
        "# Step 3: Normalize for cosine similarity\n",
        "faiss.normalize_L2(query_embedding)\n",
        "\n",
        "# Step 4: Search FAISS index for top-k (e.g., 5) most similar passages\n",
        "top_k = 5\n",
        "D, I = index.search(query_embedding, top_k)\n",
        "\n",
        "# D = similarity scores, I = indices of top matches\n",
        "print(\"Top retrieved passages:\")\n",
        "for i, idx in enumerate(I[0]):\n",
        "    print(f\"\\nPassage {i+1} (score={D[0][i]:.4f}):\")\n",
        "    print(df['passage'].iloc[idx])\n"
      ],
      "metadata": {
        "id": "4u0KVX5CdMwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Answer Using a Pretrained Model (e.g., BART or T5)"
      ],
      "metadata": {
        "id": "Serso1D7dMsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "model_name = \"facebook/bart-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Zyf18F21dk1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse previous query\n",
        "query = \"How does COVID-19 spread?\"\n",
        "\n",
        "# Get top 1 retrieved passage\n",
        "retrieved_passage = df['passage'].iloc[I[0][0]]\n",
        "\n",
        "# Concatenate passage and question as input\n",
        "input_text = f\"{retrieved_passage} \\n\\n Question: {query}\"\n"
      ],
      "metadata": {
        "id": "zvtvyWfQdkyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "retrieved_passage = df['passage'].iloc[I[0][0]]\n",
        "I is the result from your FAISS search:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "D, I = index.search(query_embedding, top_k)\n",
        "I[0] is the list of top-k document indices for your first (and only) query.\n",
        "\n",
        "I[0][0] is the index of the most similar passage (rank 1)."
      ],
      "metadata": {
        "id": "qm5ga4C7eMO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "output_ids = model.generate(**inputs, max_new_tokens=100)\n",
        "\n",
        "# Decode and print\n",
        "answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "AF4YLJGZd508"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n"
      ],
      "metadata": {
        "id": "qaJ9GQ8YfoAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "bleu = evaluate.load('bleu')\n",
        "\n",
        "# Example: lists of predictions and references\n",
        "predictions = [\"COVID-19 spreads through droplets.\"]\n",
        "references = [[\"COVID-19 is spread by droplets.\"]]\n",
        "\n",
        "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
        "bleu_results = bleu.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(\"ROUGE:\", rouge_results)\n",
        "print(\"BLEU:\", bleu_results)\n"
      ],
      "metadata": {
        "id": "yptD105WfT6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-spmG97fOA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}