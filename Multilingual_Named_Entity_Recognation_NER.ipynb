{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPopT41USyfH1GPopMy1Cwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/Multilingual_Named_Entity_Recognation_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multilingual Named Entity Recognition"
      ],
      "metadata": {
        "id": "emwWNtDTMfWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "qCNH5w4XMs84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets transformers fsspec huggingface_hub"
      ],
      "metadata": {
        "id": "WsjBQNSpM90c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "load_dataset('xtreme',name='PAN-X.de')"
      ],
      "metadata": {
        "id": "kmfiZFWsNCZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from datasets import DatasetDict #dataset strukturu:train,valid,test kimi olan daatsetler ucun\n",
        "\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"] #4 dil\n",
        "fracs = [0.629, 0.229, 0.084, 0.059] #her dil ucun nece faiz data secilecek\n",
        "panx_ch = defaultdict(DatasetDict)#hər bir dil üçün bir DatasetDict (train/validation/test) saxlayacaq.\n",
        "\n",
        "\n",
        "#Bu kod, hər bir dil üçün PAN-X datasını yükləyir, train, validation, test hissələrini qarışdırır və onlardan yalnız müəyyən faiz (frac qədər)\n",
        "# nümunə seçərək panx_ch adlı sözlükdə saxlayır.\n",
        "for lang, frac in zip(langs, fracs):\n",
        "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "    for split in ds:\n",
        "        panx_ch[lang][split] = (\n",
        "            ds[split]\n",
        "            .shuffle(seed=0)\n",
        "            .select(range(int(frac * ds[split].num_rows))))"
      ],
      "metadata": {
        "id": "3jeaLdDkNCWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "panx_ch['de']['train'][0]"
      ],
      "metadata": {
        "id": "VtbwmS8iNCS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "panx_ch"
      ],
      "metadata": {
        "id": "gg1sF8twNCP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame({lang:[panx_ch[lang]['train'].num_rows] for lang in langs}, #her language den nece example gelir\n",
        "             index=['Number of training examples'])"
      ],
      "metadata": {
        "id": "rh1W50ufNLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Alman dilindəki train datasından ilk nümunənin tərkibində nə olduğunu (məsələn, hansı sözlər və etiketlər) göstərməkdir.\n",
        "element=panx_ch['de']['train'][0]\n",
        "for key,value in element.items():\n",
        "    print(f'{key}:{value}')"
      ],
      "metadata": {
        "id": "ec_oEj6kNLAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags=panx_ch['de']['train'].features['ner_tags'].feature  #taglara baxiriq\n",
        "tags\n",
        "#features → Bu datasetdəki bütün sütunların (features) xüsusiyyətlərini (metadata) saxlayan obyekt."
      ],
      "metadata": {
        "id": "oAkT7IsYNK81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tag_names(batch):\n",
        "    return{'ner_tags_str':[tags.int2str(idx) for idx in batch['ner_tags']]}\n",
        "panx_de=panx_ch['de'].map(create_tag_names)\n",
        "#Alman dilindəki datasetdə olan ner_tags sütunundakı rəqəmləri onların müvafiq etiket adlarına\n",
        "#(məsələn, B-PER, I-LOC, O və s.) çevirir və yeni ner_tags_str sütunu kimi əlavə edir."
      ],
      "metadata": {
        "id": "Tco_UlJWNTyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hazirlamag idi gelen dersden modeli dzeldeceyik"
      ],
      "metadata": {
        "id": "VQwks08bNTvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KiY93X0SqYC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual Transformers"
      ],
      "metadata": {
        "id": "QPZGUci6NaEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_model_name='bert-base-cased' #uncased boyuk balaca herflere fikir vermir,cased fikir verir.\n",
        "xlmr_model_name='xlm-roberta-base'# bertle eyni architektura var ama xlmr multilanguagedir coxlu dil ustunde trainolub ama bert ing dilde,\n",
        "#xlmr-sentencepiece tokenizeer edir ama bert wordpiece\n",
        "bert_tokenizer=AutoTokenizer.from_pretrained(bert_model_name)\n",
        "xlmr_tokenizer=AutoTokenizer.from_pretrained(xlmr_model_name)"
      ],
      "metadata": {
        "id": "ELPo7RjFNmyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='Jack Sparrow love New York'\n",
        "bert_tokens=bert_tokenizer(text).tokens()\n",
        "xlmr_tokens=xlmr_tokenizer(text).tokens()#senetnce piece <s> </s>\n",
        "#normalization boyu-->balaca , eyni formata salir , boshluqlari silir,\n",
        "#pretokenizer boshluqlara gore, xlm de yoxdu\n",
        "#postprocessing bizim basha dusheeyimiz dilde"
      ],
      "metadata": {
        "id": "HLa_3xDYNqy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame([bert_tokens, xlmr_tokens],\n",
        "                 index=['Bert','XLM-R']) #yanida hecne yoxdurss_\n",
        "#robert-->robust fln\n",
        "df"
      ],
      "metadata": {
        "id": "liF3LelkNqwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zEp2Yh1jqc_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SentencePiece Tokenizer"
      ],
      "metadata": {
        "id": "PpFIjX7qNqti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(xlmr_tokens).replace('\\u2581',\" \") #bu _ xettin kodud unicod \\u2581"
      ],
      "metadata": {
        "id": "bDiHyw3FNqqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K71OG8fgqdyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Custom Model for Token Classification"
      ],
      "metadata": {
        "id": "5DgiRZI_NqnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers import XLMRobertaModel\n",
        "from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n",
        "from transformers import Trainer # Make sure Trainer is imported here if not already"
      ],
      "metadata": {
        "id": "rb2NmGLbN797"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Head--> Ber for ne uchun Body oyrenib Head 0 dan  , onu oyredirik, bizde HEAd olamsa?? HUggingFace 'token classificton'--body+head-NER-di"
      ],
      "metadata": {
        "id": "17YBp6PaaaOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    config_class = XLMRobertaConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = XLMRobertaModel(config, add_pooling_layer=False)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None, num_items_in_batch=None, **kwargs):\n",
        "        # Added num_items_in_batch to the forward method signature\n",
        "        # and it will be captured by **kwargs before being passed to the internal model.\n",
        "        # We don't need to explicitly use num_items_in_batch here for this model.\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask, **kwargs)\n",
        "        sequence_output = self.dropout(outputs[0])\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions\n",
        "        )\n",
        "\n",
        "# Rest of your code remains the same\n",
        "# You can now rerun the cell with trainer.train()"
      ],
      "metadata": {
        "id": "chzh6i5wN-UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "naKKBeF5qjBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a Custom Model"
      ],
      "metadata": {
        "id": "CMLXBWnLOBI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag={idx:tag for idx,tag in enumerate(tags.names)}\n",
        "tag2index={tag:idx for idx,tag in enumerate(tags.names)}"
      ],
      "metadata": {
        "id": "-yegxFioOBF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2index"
      ],
      "metadata": {
        "id": "5c6EoMWfOBCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag"
      ],
      "metadata": {
        "id": "VxKZyY1tOA9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags.names"
      ],
      "metadata": {
        "id": "9UEcp99mOJ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "xlmr_config=AutoConfig.from_pretrained(xlmr_model_name,num_labels=tags.num_classes,\n",
        "                                       id2label=index2tag,label2id=tag2index)"
      ],
      "metadata": {
        "id": "b-tQklA0OJ5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "xlmr_model=XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name,config=xlmr_config).to(device)"
      ],
      "metadata": {
        "id": "WsZCu2S6OJ10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#datani hazirla\n",
        "input_ids=xlmr_tokenizer.encode(text, return_tensors='pt') #encode ne dirdi?? niye bele\n",
        "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=['Tokens','Input IDs'])"
      ],
      "metadata": {
        "id": "hmyVJMAQOJy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=xlmr_model(input_ids.to(device)).logits\n",
        "predictions=torch.argmax(outputs,dim=-1)\n",
        "print(f'Number of tokens in sequence: {len(predictions[0])}')\n",
        "print(f'Shape of outputs tensor: {outputs.shape}')"
      ],
      "metadata": {
        "id": "IT8adPYGOQRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "VT5nj5S8OQNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "8bH919JTOQJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds=[tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "pd.DataFrame([xlmr_tokens,preds],index=['Tokens','Tags'])"
      ],
      "metadata": {
        "id": "AdGRprx_OQF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags.names"
      ],
      "metadata": {
        "id": "d0ObjvI3OW-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_text(text, tags, model, tokenizer):#kecendefeki\n",
        "    tokens = tokenizer(text).tokens() #Bu isə, yuxarıdakı tokenizer nəticəsindən sadəcə tokenləri çıxarır (yəni [‘play’, ‘##ing’] kimi nəticə verir).\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')#reqem kim --Bu hissə text mətnini alır və onu token ID-lərinə çevirir.Yəni sözlər → subword tokenlər → rəqəmlər (ID-lər)\n",
        "    outputs = model(input_ids.to(device))[0] #last_hidden_state\n",
        "    predictions = torch.argmax(outputs, dim=-1) #calssi verir yeni BIO-da hansi calssa argmaxi coxdu--> bu\n",
        "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n",
        "# text = \"Obama\"\n",
        "# tokens = ['O', '##bam', '##a']\n",
        "# input_ids = [101, 1234, 5678, 102]\n",
        "#outputs=outputs.shape = [batch_size, sequence_length, num_tags]\n",
        "#predictions = [[0, 2, 2, 0, 4, 5, 0, 3, 3, 0]]\n",
        "# predictions = [0, 2, 2, 1]\n",
        "# tags.names = ['O', 'B-PER', 'I-PER']\n",
        "# preds = ['O', 'I-PER', 'I-PER', 'B-PER']"
      ],
      "metadata": {
        "id": "s-r6DGHFa4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. tokenizer(text)\n",
        "    # Bu hissə, tokenizer obyektinə text adlı mətni verir və tokenizer onu emal edir.\n",
        "    # Məsələn: text = \"playing\"\n",
        "    # Tokenizer: WordPiece, Unigram və s.\n",
        "    # Nəticə: Tokenizer obyektindən çıxan bir Encoded object (məs: Encoding).\n",
        "\n",
        "# output = (\n",
        "#     last_hidden_state,     # [0] → əsas nəticə (ən çox istifadə olunan)\n",
        "#     pooled_output,         # [1] → CLS tokenin çıxışı (ən çox klassifikasiya üçün istifadə olunur)\n",
        "#     hidden_states (optional),\n",
        "#     attentions (optional)\n",
        "# )"
      ],
      "metadata": {
        "id": "e35UNsS-a5-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaCwIxD6qkF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing Texts for NER"
      ],
      "metadata": {
        "id": "roNEMYNha57O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "de_example = panx_de[\"train\"][0]\n",
        "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]],\n",
        "['Tokens', 'Tags'])"
      ],
      "metadata": {
        "id": "XsTI2b65a54L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words, labels=de_example['tokens'], de_example['ner_tags']"
      ],
      "metadata": {
        "id": "Eg4_M86La50w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "id": "1-Ve8iXSa5xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input=xlmr_tokenizer(words, is_split_into_words=True) #sozlere bolunu b yoxsa yox, string olara false\n",
        "tokens=xlmr_tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
        "pd.DataFrame([tokens,labels], index=['Tokens','Labels'])#sozler bolunu  label ile solzer ust uste duhmur"
      ],
      "metadata": {
        "id": "mYzq5O5mbEhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input #etxt--reqem"
      ],
      "metadata": {
        "id": "geK5VstdbHNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input.word_ids()"
      ],
      "metadata": {
        "id": "h34prfXMbL5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids=tokenized_input.word_ids()\n",
        "pd.DataFrame([tokens, word_ids], index=['Tokens','Word IDs'])# eyni soelr yeni reqemlerle, NONElere -100 deyeceyik,  pytorchda -100 ignore edir"
      ],
      "metadata": {
        "id": "mH1L6X4ubNZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_word_idx=None\n",
        "label_ids=[]\n",
        "\n",
        "for word_idx in word_ids:\n",
        "    if word_idx is None or word_idx==previous_word_idx:\n",
        "        label_ids.append(-100)\n",
        "    else:\n",
        "        label_ids.append(labels[word_idx])\n",
        "    previous_word_idx=word_idx\n",
        "\n",
        "labels=[index2tag[l] if l!=-100 else 'IGN' for l in label_ids]\n",
        "index=['Tokens','Word IDs','Label IDs','Labels']\n",
        "df=pd.DataFrame([tokens, word_ids,label_ids, labels], index=index)\n",
        "df"
      ],
      "metadata": {
        "id": "vCQPAUtebb32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_ids"
      ],
      "metadata": {
        "id": "IqaA2e-AbTi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag"
      ],
      "metadata": {
        "id": "RUsI2NAPbXrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = xlmr_tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    all_labels = []\n",
        "\n",
        "    for i in range(len(examples[\"tokens\"])):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        labels = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None or word_idx == previous_word_idx:\n",
        "                labels.append(-100)\n",
        "            else:\n",
        "                labels.append(examples[\"ner_tags\"][i][word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = all_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "def encode_panx_dataset(corpus):\n",
        "    encoded_corpus = DatasetDict()\n",
        "    for split, dataset in corpus.items():\n",
        "        encoded_corpus[split] = dataset.map(\n",
        "            tokenize_and_align_labels,\n",
        "            batched=True,  # Doğrudur burada\n",
        "            remove_columns=[\"langs\", \"ner_tags\", \"tokens\"]\n",
        "        )\n",
        "    return encoded_corpus\n",
        "\n",
        "\n",
        "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])\n",
        "print(panx_de_encoded[\"train\"][0])"
      ],
      "metadata": {
        "id": "TVvguqHgbdTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "id": "nm9zymm_bf8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
        "\n",
        "# You can now use the corrected function:\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "6wW5RKxjbiC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #hugging face list ichind elist gozleyir\n",
        "def align_predictions(predictions, label_ids):\n",
        "    preds=np.argmax(predictions, axis=2)\n",
        "    batch_size, seq_len=preds.shape\n",
        "    label_list, preds_list=[],[]\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "        example_labels, example_preds=[],[]\n",
        "        for seq_idx in range(seq_len):\n",
        "            if label_ids[batch_idx, seq_idx]!=-100:\n",
        "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
        "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
        "        label_list.append(example_labels)\n",
        "        preds_list.append(example_preds)\n",
        "    return preds_list, label_list"
      ],
      "metadata": {
        "id": "Ikrbg-nAbj2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82zZ_Jg0qnTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning XLM-RoBerta"
      ],
      "metadata": {
        "id": "ZYNFjILBbtsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "num_epochs=3\n",
        "batch_size=24\n",
        "logging_steps=len(panx_de_encoded['train'])//batch_size\n",
        "\n",
        "model_name=f'{xlmr_model_name}-finetuned-panx-de'\n",
        "training_args=TrainingArguments(\n",
        "    output_dir=model_name, log_level='error', num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    eval_strategy='epoch',\n",
        "    save_steps=1e6, #100 000 addimdan bir save etsin yeni etmesin  her iterasiyada  save etmek istemirk, istesek epoch yaz\n",
        "    weight_decay=0.01,\n",
        "    disable_tqdm=False,\n",
        "    logging_steps=logging_steps,\n",
        "    push_to_hub=True\n",
        ")"
      ],
      "metadata": {
        "id": "yh91Q4q-bu5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "OEepXlt0bxKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    y_pred, y_true=align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
        "    return {'f1':f1_score(y_true, y_pred)} #butun duhmeldui ust uste"
      ],
      "metadata": {
        "id": "rBrW2ZLgby4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification #Ner uchun Pad etmeye, her defe ramd ayer tutur model , funkdiyani ichine yukleyek modlei ordan caqirsin\n",
        "\n",
        "# Correct the typo in the class name\n",
        "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "Mz3HsfRucaCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_init():\n",
        "    return XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name,\n",
        "                                                            config=xlmr_config).to(device)"
      ],
      "metadata": {
        "id": "W-zP49F6cbXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer=Trainer(model_init=model_init,\n",
        "                args=training_args, #compile\n",
        "                data_collator=data_collator,#nece pad\n",
        "                compute_metrics=compute_metrics, #hani metrics\n",
        "                train_dataset=panx_de_encoded['train'],\n",
        "                eval_dataset=panx_de_encoded['validation'],\n",
        "                tokenizer=xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "wPLaVw9Tcc_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "# trainer.push_to_hub(config)"
      ],
      "metadata": {
        "id": "-HDSzjYDcebz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "9JYPL9-Dcg_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(trainer.state.log_history)[['epoch', 'loss', 'eval_loss', 'eval_f1']]\n",
        "df = df.rename(columns={'eval_loss':'Validation_loss',\n",
        "                      'loss':'Training Loss', 'epoch':'Epoch', 'eval_f1':'F1'})\n",
        "df['Epoch'] = df['Epoch'].apply(lambda x:round(x))\n",
        "df['Training Loss'] = df['Training Loss'].ffill()\n",
        "df[['Validation_loss','F1']] = df[['Validation_loss','F1']].bfill().ffill()\n",
        "df.drop_duplicates()"
      ],
      "metadata": {
        "id": "ywF5HtnU3JCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_de = \"Jack Dean ist ein Informatiker bei Google in Kallifornien\"\n",
        "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "QLnRdbuD37ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hT5EUyQsqqNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Analysis"
      ],
      "metadata": {
        "id": "bSFVRZc237fF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "def forward_pass_with_label(batch):\n",
        "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
        "    batch = data_collator(features)\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    with torch.no_grad():\n",
        "        output = trainer.model(input_ids, attention_mask)\n",
        "        predicted_label = torch.argmax(output.logits, axis = -1).cpu().numpy()\n",
        "\n",
        "        loss = cross_entropy(output.logits.view(-1, 7),\n",
        "                             labels.view(-1), reduction = 'none')\n",
        "        loss = loss.view(len(input_ids), -1).cpu().numpy()\n",
        "\n",
        "        return {'loss': loss, 'predicted_label': predicted_label}"
      ],
      "metadata": {
        "id": "1LGXWZq93-cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_set = panx_de_encoded['validation']\n",
        "valid_set = valid_set.map(forward_pass_with_label, batched = True, batch_size = 32)\n",
        "df = valid_set.to_pandas()\n",
        "df"
      ],
      "metadata": {
        "id": "ahN9MyVB7kEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag[-100] = 'IGN'\n",
        "\n",
        "df['input_tokens'] = df['input_ids'].apply(lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\n",
        "df['predicted_label'] = df['predicted_label'].apply(lambda x: [index2tag[i] for i in x])\n",
        "df['labels'] = df['labels'].apply(lambda x: [index2tag[i] for i in x])\n",
        "df['loss'] = df.apply(lambda x: x['loss'][:len(x['input_ids'])], axis = 1)\n",
        "df['predicted_label'] = df.apply(lambda x: x['predicted_label'][:len(x['input_ids'])], axis = 1)\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "I3s3UbTT8FOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokens = df.apply(pd.Series.explode)\n",
        "df_tokens = df_tokens.query(\"labels != 'IGN'\")\n",
        "df_tokens['loss'] = df_tokens['loss'].astype(float).round(2)\n",
        "df_tokens.head(7)"
      ],
      "metadata": {
        "id": "Te-zRsyU-gGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    df_tokens.groupby('input_tokens')[['loss']]\n",
        "    .agg(['count', 'mean', 'sum'])\n",
        "    .droplevel(level = 0, axis = 1)\n",
        "    .sort_values(by = 'sum', ascending = False)\n",
        "    .reset_index()\n",
        "    .head(10)\n",
        "    .T\n",
        ")"
      ],
      "metadata": {
        "id": "qjr3-Zfu_YgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    df_tokens.groupby('labels')[['loss']]\n",
        "    .agg(['count', 'mean', 'sum'])\n",
        "    .droplevel(level = 0, axis = 1)\n",
        "    .sort_values(by = 'sum', ascending = False)\n",
        "    .reset_index()\n",
        "    .round(2)\n",
        "    .T\n",
        ")"
      ],
      "metadata": {
        "id": "dqo8pLZ1_-oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1-np.log(1/7) #bu qeder sehv ederdi eger hecne oyrenmeseydi"
      ],
      "metadata": {
        "id": "LKOeoxlRAyaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(y_preds, y_true, labels):\n",
        "    cm = confusion_matrix(y_true, y_preds, normalize = 'true')\n",
        "    fig, ax = plt.subplots(figsize = (6, 6))\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = labels)\n",
        "    disp.plot(cmap = plt.cm.Purples, values_format = '.2f', ax = ax, colorbar = False)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6tEFOSR9CLFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(df_tokens['predicted_label'],\n",
        "                      df_tokens['labels'], tags.names)"
      ],
      "metadata": {
        "id": "klgPAF8tCP4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_samples(df):\n",
        "    for _, row in df.iterrows():\n",
        "        labels, preds, tokens, losses = [], [], [], []\n",
        "        for i, mask in enumerate(row['attention_mask']):\n",
        "            if i not in {0, len(row['attention_mask']) - 1}:\n",
        "                labels.append(row['labels'][i])\n",
        "                preds.append(row['predicted_label'][i])\n",
        "                tokens.append(row['input_tokens'][i])\n",
        "                losses.append(f\"{row['loss'][i]:.2f}\")\n",
        "\n",
        "        df_tmp = pd.DataFrame({'tokens': tokens, 'labels': labels,\n",
        "                               'preds' : preds, 'losses': losses}).T\n",
        "        yield df_tmp\n",
        "\n",
        "df['total_loss'] = df['loss'].apply(sum)\n",
        "df_tmp = df.sort_values(by = 'total_loss', ascending = False).head(3)\n",
        "\n",
        "for sample in get_samples(df_tmp):\n",
        "    display(sample)\n",
        "#silver label - basga model terefinden labellanib\n",
        "#golden label - insan terefinden labellanib"
      ],
      "metadata": {
        "id": "mSPLBY9fCeGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cx3yEAOLqtUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross-Lingual Transfer"
      ],
      "metadata": {
        "id": "XmyVRAZiEaq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_f1_score(trainer, dataset):\n",
        "    return trainer.predict(dataset).metrics['test_f1']"
      ],
      "metadata": {
        "id": "jLe-rIL2F84x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores = defaultdict(dict) #adi dict kimi, yoxdusa error qaytarmir\n",
        "f1_scores['de']['de'] = get_f1_score(trainer, panx_de_encoded['test'])\n",
        "print(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")"
      ],
      "metadata": {
        "id": "pdT12CK8GE1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_fr = \"Jeff Dean est informaticien ches Google en Californie\"\n",
        "tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)"
      ],
      "metadata": {
        "id": "soL0Kkc7GYQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_lang_performance(lang, trainer):\n",
        "    panx_ds = encode_panx_dataset(panx_ch[lang])\n",
        "    return get_f1_score(trainer, panx_ds['test'])"
      ],
      "metadata": {
        "id": "_BqFVuEqHun2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores['de']['fr'] = evaluate_lang_performance('fr', trainer)\n",
        "print(f\"F1-score pf [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")"
      ],
      "metadata": {
        "id": "OKe08NqnH_Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores['de']['it'] = evaluate_lang_performance('it', trainer)\n",
        "print(f\"F1-score pf [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")"
      ],
      "metadata": {
        "id": "8A7jzcHGIOvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores['de']['en'] = evaluate_lang_performance('en', trainer)\n",
        "print(f\"F1-score pf [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")"
      ],
      "metadata": {
        "id": "u5nlCdjxIl9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "panx_fr_encoded = encode_panx_dataset(panx_ch['fr'])"
      ],
      "metadata": {
        "id": "4g2mW9QNIqPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#when does zero-shot transfer make sense?"
      ],
      "metadata": {
        "id": "DWUtBXhgrRP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_on_subset(dataset, num_samples):\n",
        "    train_ds=dataset['train'].shuffle(seed=42).select(range(num_samples))\n",
        "    valid_ds=dataset['validation']\n",
        "    test_ds=dataset['test']\n",
        "    training_args.logging_steps=len(train_ds)//batch_size\n",
        "\n",
        "    trainer=Trainer(model_init=model_init, args=training_args,\n",
        "                    data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "                    train_dataset=train_ds, eval_dataset=valid_ds,\n",
        "                    processing_class=xlmr_tokenizer)\n",
        "\n",
        "    trainer.train()\n",
        "    if training_args.push_to_hub:\n",
        "        trainer.push_to_hub(commit_message=\"Training completed!\")\n",
        "\n",
        "    f1_score=get_f1_score(trainer, test_ds)\n",
        "    return pd.DataFrame.from_dict({\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]})"
      ],
      "metadata": {
        "id": "kSa5gFazvpfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args.push_to_hub = False\n",
        "metrics_df = train_on_subset(panx_fr_encoded, 250)\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "06pdC1wsrfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num_samples in [500, 1000, 2000, 4000]:\n",
        "    metrics_df = pd.concat([metrics_df, train_on_subset(panx_fr_encoded, num_samples)],\n",
        "                           ignore_index = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "q1N2IPAFnMgw",
        "outputId": "e6445941-c234-4779-f9a7-3d13dc972a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 01:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.426300</td>\n",
              "      <td>0.970359</td>\n",
              "      <td>0.317760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.826400</td>\n",
              "      <td>0.656759</td>\n",
              "      <td>0.603703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 02:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.426300</td>\n",
              "      <td>0.970359</td>\n",
              "      <td>0.317760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.826400</td>\n",
              "      <td>0.656759</td>\n",
              "      <td>0.603703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.568100</td>\n",
              "      <td>0.556923</td>\n",
              "      <td>0.664750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='127' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 02:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.102700</td>\n",
              "      <td>0.530733</td>\n",
              "      <td>0.684364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.473300</td>\n",
              "      <td>0.404460</td>\n",
              "      <td>0.734747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.axhline(f1_score['de']['fr']ls = \"--\", color = \"r\")\n",
        "metrics_df.set_index(\"num_samples\").plot(ax = ax)\n",
        "plt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc = \"lower right\")\n",
        "plt.ylim((0, 1))\n",
        "plt.xlabel(\"Number of Training Samples\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GEXweNCFnMdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SdqBeKvxqwHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning on Multiple Languages at once"
      ],
      "metadata": {
        "id": "voOKktucoild"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "def concatenate_splits(corpora):\n",
        "    multi_corpus = DatasetDict()\n",
        "    for split in corpora[0].keys():\n",
        "        multi_corpus[split] = concatenate_datasets(\n",
        "            [corpus[split] for corpus in corpora]).shuffle(seed = 42)\n",
        "    return multi_corpus"
      ],
      "metadata": {
        "id": "6WTYVfIspADo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "panx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])"
      ],
      "metadata": {
        "id": "scFWko1lpZJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args.logging_steps = len(panx_de_fr_encoded['train']) // batch_size\n",
        "training_args.push_to_hub = False\n",
        "training_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n",
        "\n",
        "trainer = Trainer(model_init = model_init, args = training_args,\n",
        "                  data_collator = data_collator, compute_metrics = compute_metrics,\n",
        "                  preprocessing_class = xlmr_tokenizer, train_dataset = panx_de_fr_encoded['train'],\n",
        "                  eval_dataset = panx_de_fr_encoded['validation'])\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "BAEAYzQArEYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lang in langs:\n",
        "    f1 = evaluate_lang_performance(lang, trainer)\n",
        "    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "4l0Aq9iJryny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8UIrllRDscfm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}