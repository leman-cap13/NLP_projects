{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0SLo8yaeBUCJ/v2Jn4YE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpIQEeXLU1O7"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "269ZuUVEU4Sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for review,label in raw_train_set.take(4):\n",
        "#   print(review.numpy().decode('utf-8')[:200],'...')\n",
        "#   print('label:', label.numpy())"
      ],
      "metadata": {
        "id": "2lT1tcFuV9b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_set, raw_valid_set, raw_test_set=tfds.load(name='imdb_reviews',\n",
        "                                                     split=['train[:90%]', 'train[90%:]','test'],\n",
        "                                                     as_supervised=True)\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "train_set=raw_train_set.shuffle(5000,seed=42).batch(32).prefetch(1)\n",
        "valid_set=raw_valid_set.batch(32).prefetch(1)\n",
        "test_set=raw_test_set.batch(32).prefetch(1)"
      ],
      "metadata": {
        "id": "NbENp06XU7NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for review,label in raw_train_set.take(4):\n",
        "  print(review.numpy().decode('utf-8')[:200],'...')\n",
        "  print('label:', label.numpy())"
      ],
      "metadata": {
        "id": "RJiSSYmAWVUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=1000\n",
        "\n",
        "text_vec_layer=tf.keras.layers.TextVectorization(max_tokens=vocab_size,standardize='lower_and_strip_punctuation', split='whitespace')\n",
        "\n",
        "text_vec_layer.adapt(train_set.map(lambda review,label:review))"
      ],
      "metadata": {
        "id": "kqeaqWYPWYEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer.get_vocabulary()[-50:]"
      ],
      "metadata": {
        "id": "FVjFkFOzWYAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(map(str,text_vec_layer.get_vocabulary()[:50]))"
      ],
      "metadata": {
        "id": "yW9-SORKYn2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence=text_vec_layer(['it is an awful movie', 'it is'])\n",
        "sample_sentence"
      ],
      "metadata": {
        "id": "x8Ip7Ag4ZjYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer=tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64)\n",
        "embed_layer(sample_sentence)"
      ],
      "metadata": {
        "id": "R7xJ42mHZjUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer(sample_sentence[0,0])"
      ],
      "metadata": {
        "id": "BMquOfi4ZjSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size=128\n",
        "model=tf.keras.models.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size,embed_size,mask_zero=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "\n",
        "])\n"
      ],
      "metadata": {
        "id": "4cZxXngEbhih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Nadam(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "A1dgEe2pbhfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_set, epochs=3,validation_data=valid_set)"
      ],
      "metadata": {
        "id": "yrVZgzzOcekd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights=model.layers[1].get_weights()[0]"
      ],
      "metadata": {
        "id": "cliUfaj7lnCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YNZjQNLrmTGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('embeddings.tsv', embedding_weights, delimiter='\\t')"
      ],
      "metadata": {
        "id": "wuLJiHqwmUqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=text_vec_layer.get_vocabulary()\n",
        "\n",
        "with open('metadata.tsv', 'w', encoding='utf-8') as f:\n",
        "  for word in vocab:\n",
        "    word=word if word.strip()!='' else '<PAD>'\n",
        "    f.write(f\"{word}\\n\")"
      ],
      "metadata": {
        "id": "GrdyN7CMmfIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Shakespearean Text Using A character RNN"
      ],
      "metadata": {
        "id": "nV1LWd2anArG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "g1YguWkSptt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text=f.read()"
      ],
      "metadata": {
        "id": "KqkpX0Qwpw_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:80])"
      ],
      "metadata": {
        "id": "xiC8HZuhqIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(sorted(set(shakespeare_text)))"
      ],
      "metadata": {
        "id": "z4A02f5TqKSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "metadata": {
        "id": "9k5IdoYtqKO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer=tf.keras.layers.TextVectorization(split='character',standardize='lower')\n",
        "\n",
        "text_vec_layer.adapt(shakespeare_text)\n",
        "encode=text_vec_layer([shakespeare_text][0])"
      ],
      "metadata": {
        "id": "IC23JoJJq3E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode"
      ],
      "metadata": {
        "id": "jx2CUXSLrW-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode-=2\n",
        "vocab_size=text_vec_layer.vocabulary_size()-2\n",
        "dataset_size=len(encode)"
      ],
      "metadata": {
        "id": "RymZQHwIrdKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer.vocabulary_size()-2"
      ],
      "metadata": {
        "id": "Ag6BH-Par34Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size"
      ],
      "metadata": {
        "id": "Y85vfnZRsUmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, shuffle=False,seed=None, batch_size=32):\n",
        "  ds=tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds=ds.window(length+1, shift=1,drop_remainder=True)\n",
        "  ds=ds.flat_map(lambda window_ds: window_ds.batch(length+1))\n",
        "  if shuffle:\n",
        "    ds=ds.shuffle(100_000,seed=seed)\n",
        "  ds=ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:,:-1], window[:,1:])).prefetch(1)\n"
      ],
      "metadata": {
        "id": "i1dunvSQskIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length=100\n",
        "tf.random.set_seed(42)\n",
        "train_set=to_dataset(encode[:1_000_000],length=length, shuffle=True, seed=42)\n",
        "\n",
        "valid_set=to_dataset(encode[1_000_000:1_060_000], length=length)\n",
        "test_set=to_dataset(encode[1_060_000:],length=length)"
      ],
      "metadata": {
        "id": "qC1w6uHPubjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "metadata": {
        "id": "xIhTwrvBw514"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQy9K6_OulWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "model=tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size,output_dim=16),\n",
        "    tf.keras.layers.GRU(128,return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "model_ckpt=tf.keras.callbacks.ModelCheckpoint('my_shakespeare_model.keras', monitor='val_accuracy', save_best_only=True)\n"
      ],
      "metadata": {
        "id": "jNpRyA0rw75W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(train_set, validation_data=valid_set, epochs=1, callbacks=[model_ckpt])"
      ],
      "metadata": {
        "id": "mbwPzihiwYEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model=tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda x:x-2),\n",
        "    model\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "IXnfbN1hwZYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba=shakespeare_model.predict(tf.constant(['to be or not to b']))[0,-1]\n",
        "y_pred=tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred+2]"
      ],
      "metadata": {
        "id": "PM0LLy0wxvcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba=shakespeare_model.predict(tf.constant(['I am to your father to the duke']))[0,-1]\n",
        "y_pred=tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred+2]"
      ],
      "metadata": {
        "id": "gFYpBHOtzuU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Geenerating Fake Shakespeare text"
      ],
      "metadata": {
        "id": "pq15GcdfyOU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probas=tf.math.log([[0.5,0.4,0.1]])\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas,num_samples=8)"
      ],
      "metadata": {
        "id": "44kGA1KlyfT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "  text=tf.constant([text])\n",
        "  y_proba=shakespeare_model.predict(text)[0,-1:]\n",
        "  rescaled_logits=tf.math.log(y_proba)/temperature\n",
        "  char_id=tf.random.categorical(rescaled_logits,num_samples=1)[0,0]\n",
        "  return text_vec_layer.get_vocabulary()[char_id+2]"
      ],
      "metadata": {
        "id": "ocCmuLQVygOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extend_text(text,chars=50, temperature=1):\n",
        "  for _ in range(chars):\n",
        "    text+=next_char(text,temperature)\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "opUcH40m0uz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "extend_text('to be or not to b')"
      ],
      "metadata": {
        "id": "ZqoLkIJ10uwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "extend_text('to be or not to b', chars=100,temperature=1)"
      ],
      "metadata": {
        "id": "zOuxCvgm0uuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#An Encoder-Decoder Network for Neural Machine Translation"
      ],
      "metadata": {
        "id": "h5PGG4WL5n1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True)\n",
        "\n",
        "# Final corrected path\n",
        "spa_txt_path = Path(path).parent / \"spa-eng_extracted\" / \"spa-eng\" / \"spa.txt\"\n",
        "\n",
        "# Read the file\n",
        "text = spa_txt_path.read_text(encoding='utf-8')\n",
        "print(text[:500])  # Print first 500 characters as a quick check"
      ],
      "metadata": {
        "id": "ohWlFsb95miW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[-20])"
      ],
      "metadata": {
        "id": "gcqUCWyq5nbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "AK5xtksL8J_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=text.replace('¡','').replace('¿','')\n",
        "pairs=[line.split('\\t') for line in text.splitlines()]\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es=zip(*pairs)"
      ],
      "metadata": {
        "id": "S-WeBf1t8LyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs"
      ],
      "metadata": {
        "id": "d3Qwcwm38p7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[1]"
      ],
      "metadata": {
        "id": "KcGAKIV-9AhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(sentences_en[i],'=>', sentences_es[i])"
      ],
      "metadata": {
        "id": "2bvS_9t09MCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=1000\n",
        "max_length=50\n",
        "\n",
        "text_vec_layer_en=tf.keras.layers.TextVectorization(\n",
        "    vocab_size,output_sequence_length=max_length\n",
        "    )\n",
        "\n",
        "text_vec_layer_es=tf.keras.layers.TextVectorization(\n",
        "    vocab_size,output_sequence_length=max_length\n",
        "    )\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ],
      "metadata": {
        "id": "YC6E_9H3knzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "wfCUXwb67XeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "cVmY8kiM8IhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "traning zamani cox yavas islemenin qarsisini almaq ucun iki input bir outpur hazirlanir"
      ],
      "metadata": {
        "id": "C7nkakze-Yi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=tf.constant(sentences_en[:100_000])\n",
        "X_valid=tf.constant(sentences_en[100_000:])\n",
        "X_train_dec=tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec=tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train=text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid=text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])\n"
      ],
      "metadata": {
        "id": "suXwko7O8L3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs=tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs=tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "QZEbzX6499Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size=128\n",
        "\n",
        "encoder_input_ids=text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids=text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "decoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,embed_size,mask_zero=True)\n",
        "\n",
        "\n",
        "encoder_embeddings=encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings=decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "0THYMTQ5-0pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=tf.keras.layers.LSTM(512,return_state=True)\n",
        "encoder_outputs,*encoder_state=encoder(encoder_embeddings)"
      ],
      "metadata": {
        "id": "tsCBQE1L_mbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder=tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs=decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "NxZmhN2B_mYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer=tf.keras.layers.Dense(vocab_size,activation='softmax')\n",
        "Y_proba=output_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "C7KAFNz9_Zfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Model(inputs=[encoder_inputs,decoder_inputs], outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3, validation_data=((X_valid,X_valid_dec),Y_valid))"
      ],
      "metadata": {
        "id": "YHQVk6-3BW0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence_en):\n",
        "  translation=''\n",
        "  for word_idx in range(max_length):\n",
        "    X=tf.constant([sentence_en])\n",
        "    X_dec=tf.constant(['startofseq' + translation])\n",
        "    y_proba=model.predict((X,X_dec))[0,word_idx]\n",
        "    predicted_word_id=np.argmax(y_proba)\n",
        "    predicted_word=text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "    if predicted_word=='endofseq':\n",
        "      break\n",
        "    translation += ' ' + predicted_word\n",
        "  return translation.strip()"
      ],
      "metadata": {
        "id": "i2sAsBE9CLzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i love music')"
      ],
      "metadata": {
        "id": "ZZOwlk5lFACC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bidirectional RNN"
      ],
      "metadata": {
        "id": "CarYZVMNE_-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder=tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True)\n",
        ")"
      ],
      "metadata": {
        "id": "3am7W0QVE_8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatenateStates(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def call(self,encoder_state):\n",
        "    return [tf.concat(encoder_state[::2], axis=1),\n",
        "            tf.concat(encoder_state[1::2],axis=1)]\n",
        "\n",
        "encoder_outputs, *encoder_state=encoder(encoder_embeddings)\n",
        "concat_states= ConcatenateStates()\n",
        "encoder_state=concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "vRgPACbdN0Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder=tf.keras.layers.LSTM(512,return_sequences=True)\n",
        "decoder_outputs=decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "\n",
        "output_layer=tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "Y_proba=output_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "gQiet91wN0JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])"
      ],
      "metadata": {
        "id": "KUWL-6MlN0HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',metrics=['accuracy'])\n",
        "\n",
        "model.fit((X_train,X_train_dec),Y_train, epochs=3, validation_data=((X_valid, X_valid_dec),Y_valid))"
      ],
      "metadata": {
        "id": "oQFcn637R5JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i love music')"
      ],
      "metadata": {
        "id": "Qyb4CBbOStEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beam Search"
      ],
      "metadata": {
        "id": "zIwP4kILTHwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = tf.constant([sentence_en])  # encoder input\n",
        "    X_dec = tf.constant([\"startofseq\"])  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = tf.constant([sentence_en])  # encoder input\n",
        "            X_dec = tf.constant([\"startofseq \" + translation])  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ],
      "metadata": {
        "id": "8pcwbwqeTlM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en='I love cats and dogs'\n",
        "translate(sentence_en)"
      ],
      "metadata": {
        "id": "2mKhbzqPTsi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(sentence_en, beam_width=3, verbose=True)"
      ],
      "metadata": {
        "id": "4ZBIepqkTsfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention Mechanisms"
      ],
      "metadata": {
        "id": "Cqi0cNuiUjzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs=tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs=tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "Mx8IcXZKUjv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size=128\n",
        "\n",
        "encoder_input_ids=text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids=text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,output_dim=embed_size)\n",
        "decoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,output_dim=embed_size)\n",
        "\n",
        "encoder_embeddings=encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings=decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "lo8RYw3jWLFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder=tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
        ")\n",
        "\n",
        "encoder_outputs, *encoder_state=encoder(encoder_embeddings)\n",
        "encoder_state=concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "zvJTR9h4W4iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder=tf.keras.layers.LSTM(512,return_sequences=True)\n",
        "decoder_outputs=decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "vq6aj8bcXz6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer=tf.keras.layers.Attention()\n",
        "attention_outputs=attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer=tf.keras.layers.Dense(vocab_size,activation='softmax')\n",
        "Y_proba=output_layer(attention_outputs)"
      ],
      "metadata": {
        "id": "FaaEtYLOYHa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[Y_proba])"
      ],
      "metadata": {
        "id": "vD28qE9AYHPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "j_BrlFQxYrhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i love soccer and also goung to the beach')"
      ],
      "metadata": {
        "id": "wXkt0kxNYrCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer model"
      ],
      "metadata": {
        "id": "TaqKJ8GIYerI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "79BOjzObj3st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=10000\n",
        "max_length=50\n",
        "embed_size=128\n",
        "num_heads=5 # multi- head count\n",
        "ff_dim=512 # feed forward(dense)\n",
        "\n",
        "encoder_inputs=tf.keras.Input(shape=(None,), dtype=tf.int32, name='encoder_inputs')\n",
        "decoder_inputs=tf.keras.Input(shape=(None,), dtype=tf.int32, name='decoder_inputs')\n",
        "\n",
        "#Embedding layer\n",
        "encoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,output_dim=embed_size,mask_zero=True)\n",
        "\n",
        "decoder_embedding_layer=tf.keras.layers.Embedding(vocab_size,output_dim=embed_size, mask_zero=True)\n",
        "\n",
        "encoder_embeddings=encoder_embedding_layer(encoder_inputs)\n",
        "decoder_embeddings=decoder_embedding_layer(decoder_inputs)\n"
      ],
      "metadata": {
        "id": "eW6stBOYj3pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Positional Embedding\n",
        "pos_embedding_layer=tf.keras.layers.Embedding(max_length,embed_size)\n",
        "positions_encoder=tf.keras.layers.Lambda(lambda x: tf.range(start=0,limit=tf.shape(x)[1],\n",
        "                                                            delta=1))(encoder_inputs)\n",
        "positions_decoder=tf.keras.layers.Lambda(lambda x: tf.range(start=0,limit=tf.shape(x)[1],\n",
        "                                                            delta=1))(decoder_inputs)\n",
        "\n",
        "pos_embed_enc=pos_embedding_layer(positions_encoder)\n",
        "pos_embed_dec=pos_embedding_layer(positions_decoder)\n",
        "\n"
      ],
      "metadata": {
        "id": "B_Q2uuVlj3m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add tokens and positional Embeddings\n",
        "encoder_embed=encoder_embeddings + pos_embed_enc\n",
        "decoder_embed=decoder_embeddings + pos_embed_dec"
      ],
      "metadata": {
        "id": "ZLGksTthlznZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder self_attention\n",
        "encoder_attention=tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(encoder_embed, encoder_embed) # q k v\n",
        "encoder_attention=tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_embed + encoder_attention) #skip connection"
      ],
      "metadata": {
        "id": "U7_HF1h0lzh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Feed Forward (Dense)\n",
        "encoder_ff=tf.keras.layers.Dense(ff_dim, activation='relu')(encoder_attention)\n",
        "encoder_ff=tf.keras.layers.Dense(embed_size)(encoder_ff)\n",
        "\n",
        "encoder_outputs=tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_attention + encoder_ff) # skip connection"
      ],
      "metadata": {
        "id": "1XhBf9m3l0Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder self_attention\n",
        "causal_mask=tf.keras.layers.Lambda(lambda x: tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])),-1,0))(decoder_inputs)\n",
        "\n",
        "decoder_attention=tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(decoder_embed, decoder_embed, attention_mask=causal_mask)\n",
        "decoder_attention=tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_embed + decoder_attention)"
      ],
      "metadata": {
        "id": "9BP8U0-frLND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder Encoder cross attention\n",
        "cross_attention=tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=embed_size)(decoder_attention,encoder_outputs,encoder_outputs)\n",
        "\n",
        "decoder_cross=tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_attention + cross_attention)"
      ],
      "metadata": {
        "id": "McApx4dfsUta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder feed_forward\n",
        "decoder_ff=tf.keras.layers.Dense(ff_dim,activation='relu')(decoder_cross)\n",
        "decoder_ff=tf.keras.layers.Dense(embed_size)(decoder_ff)\n",
        "\n",
        "decoder_outputs=tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_cross + decoder_ff)"
      ],
      "metadata": {
        "id": "gV_fSUQEt0z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final output layers\n",
        "output_logits=tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder_outputs)\n",
        "\n",
        "transformer=tf.keras.Model([encoder_inputs,decoder_inputs], output_logits)"
      ],
      "metadata": {
        "id": "_THmnpect0wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model compile\n",
        "transformer.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uAH9Xz0MuwQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=15000\n",
        "max_length=50\n",
        "\n",
        "text_vec_layer_en=tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length, pad_to_max_tokens=True)\n",
        "\n",
        "text_vec_layer_es=tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length, pad_to_max_tokens=True)\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ],
      "metadata": {
        "id": "JwQBZeujvti9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_padded=tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_en(X_train).numpy(), padding='post', maxlen=max_length)\n",
        "\n",
        "X_train_dec_padded=tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_es(X_train_dec).numpy(), padding='post', maxlen=max_length)\n",
        "\n",
        "X_valid_padded=tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_en(X_valid).numpy(), padding='post', maxlen=max_length)\n",
        "\n",
        "X_valid_dec_padded=tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_es(X_valid_dec).numpy(), padding='post', maxlen=max_length)\n",
        "\n",
        "\n",
        "\n",
        "X_train_padded=tf.constant(X_train_padded)\n",
        "X_train_dec_padded=tf.constant(X_train_dec_padded)\n",
        "X_valid_padded=tf.constant(X_valid_padded)\n",
        "X_valid_dec_padded=tf.constant(X_valid_dec_padded)\n",
        "\n",
        "transformer.fit((X_train_padded, X_train_dec_padded),Y_train,epochs=3,validation_data=((X_valid_padded, X_valid_dec_padded),Y_valid))"
      ],
      "metadata": {
        "id": "qvnMcqBow7sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def translate(sentence_en):\n",
        "    # Tokenize and pad encoder input\n",
        "    X = text_vec_layer_en(tf.constant([sentence_en]))\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(X.numpy(), padding=\"post\", maxlen=max_length)\n",
        "\n",
        "    # Start token\n",
        "    start_token = text_vec_layer_es(['startofseq'])[0][0]\n",
        "    end_token = text_vec_layer_es(['endofseq'])[0][0]\n",
        "\n",
        "    # Decoder input initialized with just the start token\n",
        "    decoder_input = [start_token]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        decoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [decoder_input], maxlen=max_length, padding=\"post\"\n",
        "        )\n",
        "\n",
        "        y_proba = transformer.predict((X, decoder_input_padded), verbose=0)[0, len(decoder_input)-1]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "\n",
        "        if predicted_word_id == end_token:\n",
        "            break\n",
        "\n",
        "        decoder_input.append(predicted_word_id)\n",
        "\n",
        "    # Map tokens back to words\n",
        "    vocab = text_vec_layer_es.get_vocabulary()\n",
        "    translated_words = [vocab[token] for token in decoder_input[1:]]  # skip start token\n",
        "\n",
        "    return ' '.join(translated_words)"
      ],
      "metadata": {
        "id": "YR86_Gd2zEuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate('i like soccer and also going to the beach'))"
      ],
      "metadata": {
        "id": "oT-SQ40QzGqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6aDjzZklzq4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}