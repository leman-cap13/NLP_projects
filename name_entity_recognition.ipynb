{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/name_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aqdi-WDx1jW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x7l0GdpyKbv"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y63PIUi_yYOu"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download rohitr4307/ner-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2NK45lEygxf"
      },
      "outputs": [],
      "source": [
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtnNOe0CywKf"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/ner-dataset.zip','r') as zip_ref:\n",
        "  zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpaowh0sy9dX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/NER_Dataset.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzK1tJ0X0tue"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjsRm7sU2WX7"
      },
      "outputs": [],
      "source": [
        "df['Word'].iloc[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfZptOSp2eEO"
      },
      "outputs": [],
      "source": [
        "df['Tag'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac8XNFXo2eAu"
      },
      "outputs": [],
      "source": [
        "#B-gpe, B-tim, B-geo, B-org,I-geo, I-org, I-tim ---BIO Formats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "VOK_-H3p3b5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qHam1sO3xxW"
      },
      "outputs": [],
      "source": [
        "# First Step Tokenization\n",
        "# our input is already token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Token to token id"
      ],
      "metadata": {
        "id": "PulmMK7I3f0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6VkqnDKGqwg"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "df['Word'] = df['Word'].apply(ast.literal_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXgYs3e4Gvhd"
      },
      "outputs": [],
      "source": [
        "df['Word'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk90uLJ-HN8X"
      },
      "outputs": [],
      "source": [
        "all_tokens = [token for row in df['Word'] for token in row]\n",
        "all_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHbiD1GZHcuv"
      },
      "outputs": [],
      "source": [
        "vocab = {token: idx+1 for idx, token in enumerate(sorted(set(all_tokens)))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLAZzKzNHe0m"
      },
      "outputs": [],
      "source": [
        "vocab_size=len(vocab)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQxnoBfbHqxd"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z4xNATUq6xGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbmB0LfLD0IX"
      },
      "outputs": [],
      "source": [
        "def tokens_to_ids(token_list, vocab):\n",
        "    return [vocab.get(token, 0) for token in token_list]\n",
        "\n",
        "token_ids = tokens_to_ids(all_tokens, vocab)\n",
        "print(token_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['token_ids'] = df['Word'].apply(lambda tokens: tokens_to_ids(tokens, vocab))"
      ],
      "metadata": {
        "id": "Y31INkJJ63mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['token_ids']"
      ],
      "metadata": {
        "id": "b7Rows6G6wb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tag token id cevirmeliyik"
      ],
      "metadata": {
        "id": "oy0doKHd3sjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tag'].dtype"
      ],
      "metadata": {
        "id": "1gqHjlsL5UMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "df['Tag'] = df['Tag'].apply(ast.literal_eval)\n"
      ],
      "metadata": {
        "id": "sw4gea_y5xlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniques_tag=sorted(df['Tag'].explode().unique())\n",
        "uniques_tag"
      ],
      "metadata": {
        "id": "kg_QCfxp4tRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2id = {tag: idx for idx, tag in enumerate(uniques_tag)}\n",
        "id2tag = {idx: tag for tag, idx in tag2id.items()}"
      ],
      "metadata": {
        "id": "aH9P77Fs6CJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2id"
      ],
      "metadata": {
        "id": "OILCFxcg6OCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2tag"
      ],
      "metadata": {
        "id": "lnIgHEGU6P2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tag_ids'] = df['Tag'].apply(lambda tags: [tag2id[tag] for tag in tags])"
      ],
      "metadata": {
        "id": "3MvX_NKC6REp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tag_ids']"
      ],
      "metadata": {
        "id": "TEIX0yBo6iXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indi Token id leri paddin etmek lazimdiki hem imput hem output uzunlugu eyni olsun"
      ],
      "metadata": {
        "id": "u2aNJfp17EpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "cZT7h9eT7dDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['seq_len'] = df['token_ids'].apply(len)\n",
        "\n",
        "print(\"Max Length:\", df['seq_len'].max())"
      ],
      "metadata": {
        "id": "WIyDKmUk7uIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = int(df['seq_len'].quantile(0.95))"
      ],
      "metadata": {
        "id": "vCns2Qco70rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN"
      ],
      "metadata": {
        "id": "q7MBAvAM72PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 35\n",
        "\n",
        "df['input_ids'] = list(pad_sequences(df['token_ids'], maxlen=MAX_LEN, padding='post', value=0))\n",
        "df['label_ids'] = list(pad_sequences(df['tag_ids'], maxlen=MAX_LEN, padding='post', value=tag2id['O']))\n"
      ],
      "metadata": {
        "id": "OVdhOO_87K20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['input_ids']"
      ],
      "metadata": {
        "id": "kqFz8zSL7791"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label_ids']"
      ],
      "metadata": {
        "id": "o4XF204e7-6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train teste bolek"
      ],
      "metadata": {
        "id": "hj1o1uhB8HxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = np.array(df['input_ids'].to_list(),dtype=np.int32)\n",
        "y = np.array(df['label_ids'].to_list(),dtype=np.int32)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Mi1zPs9V8Kjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X dtype:\", X.dtype)"
      ],
      "metadata": {
        "id": "8xoI5SwBP20G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow datasina cevirek"
      ],
      "metadata": {
        "id": "vhKoxeig8V2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size)\n",
        "valid_dataset=tf.data.Dataset.from_tensor_slices((X_test,y_test)).batch(batch_size)"
      ],
      "metadata": {
        "id": "rCbMxHeY8ZUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "XI8PWEdS82so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset"
      ],
      "metadata": {
        "id": "e5WOOM7f85J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder base transformer model\n",
        "\n",
        "encoder_inputs=tf.keras.Input(shape=[None,], name='encoder_inputs')\n",
        "\n",
        "#Embedding layer\n",
        "embed_layer=tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=128, mask_zero=True, name='embed_layer')\n",
        "\n",
        "#encoder embedding\n",
        "encoder_embed=embed_layer(encoder_inputs)\n",
        "\n",
        "\n",
        "#Positional embed layer\n",
        "embed_size=128\n",
        "pos_embed_layer=tf.keras.layers.Embedding(MAX_LEN, embed_size)\n",
        "\n",
        "pos_encoder=tf.keras.layers.Lambda(lambda x: tf.range(start=0,limit=tf.shape(x)[1],\n",
        "                                                            delta=1))(encoder_inputs)\n",
        "\n",
        "positional_embedding=pos_embed_layer(pos_encoder)\n",
        "\n",
        "\n",
        "#concat tokrn and positional embedding\n",
        "encoder_embedding=positional_embedding+encoder_embed\n",
        "\n",
        "#Add multihead attentioan layer\n",
        "\n",
        "num_heads=3\n",
        "encoder_attention=tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(encoder_embedding,encoder_embedding)\n",
        "encoder_attention=tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_attention+encoder_embedding)\n",
        "\n",
        "\n",
        "# add Feed Forward (Dense) layer\n",
        "ff_dim=512\n",
        "encoder_ff=tf.keras.layers.Dense(ff_dim, activation='relu')(encoder_attention)\n",
        "encoder_ff=tf.keras.layers.Dense(embed_size)(encoder_ff)\n",
        "encoder_ff=tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoder_ff+encoder_attention)\n",
        "\n",
        "\n",
        "#Add output layer\n",
        "output_layer=tf.keras.layers.Dense(len(tag2id), activation='softmax')(encoder_ff)"
      ],
      "metadata": {
        "id": "6XyiBaVs861T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make model\n",
        "model=tf.keras.Model(inputs=encoder_inputs, outputs=output_layer)"
      ],
      "metadata": {
        "id": "BRNWUvaY_R2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model compiling\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "hojie7gcSnGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model fitting\n",
        "model.fit(train_dataset, validation_data=valid_dataset, epochs=10)"
      ],
      "metadata": {
        "id": "dS1FPR3nBfH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Max input_id:\", np.max(X))\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ],
      "metadata": {
        "id": "eXcfrcUAPgw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 5\n",
        "y_pred_probs = model.predict(valid_dataset)\n",
        "\n",
        "y_pred = np.argmax(y_pred_probs, axis=-1)\n",
        "\n",
        "for i in range(num_examples):\n",
        "    print(f\"\\nExample {i+1}\")\n",
        "    print(\"Input Tokens:\")\n",
        "    input_tokens = [token for token, id in vocab.items() if id in X_test[i]]\n",
        "    print(input_tokens)\n",
        "\n",
        "    print(\"\\nTrue Tags:\")\n",
        "    true_tags = [id2tag[id] for id in y_test[i] if id != tag2id['O']]\n",
        "    print(true_tags)\n",
        "\n",
        "    print(\"\\nPredicted Tags:\")\n",
        "    pred_tags = [id2tag[id] for id in y_pred[i] if id != tag2id['O']]\n",
        "    print(pred_tags)\n"
      ],
      "metadata": {
        "id": "8bz8Wbm_QQ4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):  # İlk 3 nümunəyə bax\n",
        "    input_ids = X_test[i]\n",
        "    true_ids = y_test[i]\n",
        "    pred_ids = y_pred[i]\n",
        "\n",
        "    print(f\"\\nExample {i+1}\")\n",
        "    print(f\"{'Token':15} {'True Tag':10} {'Predicted Tag'}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for token_id, true_id, pred_id in zip(input_ids, true_ids, pred_ids):\n",
        "        if token_id != 0:  # padding yoxdursa\n",
        "            token = [k for k, v in vocab.items() if v == token_id][0]\n",
        "            true_tag = id2tag[true_id]\n",
        "            pred_tag = id2tag[pred_id]\n",
        "            print(f\"{token:15} {true_tag:10} {pred_tag}\")\n"
      ],
      "metadata": {
        "id": "M8EEUIJhRFve"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtHsScph/ymDDDx2ug1R+C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}