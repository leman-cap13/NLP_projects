{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKE4xvYzK7aNkXS6hxvjs+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/Text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Greedy Search Decoding"
      ],
      "metadata": {
        "id": "vcVFUxlyjxg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "6mOZ6FZZkETp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "iknPJWvkk1P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name='gpt2-xl'\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "model=AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "BSf_B8H4kEQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_txt='Transformers are the'\n",
        "input_ids=tokenizer(input_txt, return_tensors='pt')['input_ids'].to(device)\n",
        "iterations=[]\n",
        "n_steps=8\n",
        "choices_per_step=5\n",
        "\n",
        "with torch.no_grad():\n",
        "  for _ in range(n_steps):\n",
        "    iteration=dict()\n",
        "    iteration['Input']=tokenizer.decode(input_ids[0])\n",
        "    output=model(input_ids = input_ids)\n",
        "    next_token_logits=output.logits[0,-1,:]\n",
        "    next_token_probs=torch.softmax(next_token_logits,dim=-1)\n",
        "    sorted_ids=torch.argsort(next_token_probs,dim=-1,descending=True)\n",
        "\n",
        "    for choice_idx in range(choices_per_step):\n",
        "      token_id=sorted_ids[choice_idx]\n",
        "      token_prob=next_token_probs[token_id].cpu().numpy()\n",
        "      token_choice=(\n",
        "          f\"{tokenizer.decode([token_id])} ({100 * token_prob: .2f}%)\"\n",
        "\n",
        "      )\n",
        "      iteration[f\"Choice {choice_idx+1}\"]=token_choice\n",
        "    input_ids=torch.cat([input_ids, sorted_ids[None, 0, None]],dim=-1)\n",
        "    iterations.append(iteration)\n",
        "pd.DataFrame(iterations)\n",
        "\n"
      ],
      "metadata": {
        "id": "HA1xn_Y1kEOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids=tokenizer(input_txt, return_tensors='pt')['input_ids'].to(device)\n",
        "output=model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "o6as5HndkELj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=128\n",
        "\n",
        "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
        "a herd of unicorns living in a remote, previously unexplored \\\n",
        "valley, in the Andes Mountains. Even more surprising to the \\\n",
        "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
        "\"\"\"\n",
        "input_ids=tokenizer(input_txt, return_tensors='pt')['input_ids'].to(device)\n",
        "output_greedy=model.generate(input_ids, max_length=max_length, do_sample=False)\n",
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "WcbSskhntede"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beam Search"
      ],
      "metadata": {
        "id": "cq3O6LuTtea3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#why we use log probability over normal probability ?\n",
        "0.5 **1024"
      ],
      "metadata": {
        "id": "eJZygtpAwu2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "sum([np.log(0.5)] * 1024)"
      ],
      "metadata": {
        "id": "HzzRgq3huN5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def log_probs_from_logits(logits,labels):\n",
        "  logp=F.log_softmax(logits,dim=-1)\n",
        "  logp_label=torch.gather(logp,2,labels.unsqueeze(2)).squeeze(-1)\n",
        "  return logp_label"
      ],
      "metadata": {
        "id": "nDHLEtOCuNjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logit---> model predictions"
      ],
      "metadata": {
        "id": "kzucpR6MuNga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch, seq_len, words  ----> dimentions of vocab size\n",
        "# batch, seq_len ----> dimentions of labels that is why we add unsqueeze(2) it will ad 1 dimention in 2 index"
      ],
      "metadata": {
        "id": "vaCGinfjxkWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_logprob(model,labels,input_len=0):\n",
        "  with torch.no_grad():\n",
        "    output=model(labels)\n",
        "    log_probs=log_probs_from_logits(\n",
        "        output.logits[:,:-1,:],labels[:,1:]\n",
        "    )\n",
        "\n",
        "    seq_log_prob=torch.sum(log_probs[:,input_len:])\n",
        "  return seq_log_prob.cpu().numpy()"
      ],
      "metadata": {
        "id": "CtbopfBHyBzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_len: because my sentence is not importand the thing we want to know what model generate"
      ],
      "metadata": {
        "id": "IqkaI2IuyBwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logp=sequence_logprob(model,output_greedy, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_greedy[0]))\n",
        "print(f\"\\nlog-prob: {logp: .2f}\")"
      ],
      "metadata": {
        "id": "KBF22JSOypSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_beam=model.generate(input_ids,max_length=max_length,num_beams=5, do_sample=False)\n",
        "logp=sequence_logprob(model,output_beam, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_beam[0]))\n",
        "print(f\"\\nlog-prob: {logp: .2f}\")"
      ],
      "metadata": {
        "id": "NclRXXu7ypOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How to prevent repeated sentences?\n",
        "#---->no_repeat_ngram_size\n",
        "output_beam=model.generate(input_ids,max_length=max_length,num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n",
        "logp=sequence_logprob(model,output_beam, input_len=len(input_ids[0]))\n",
        "print(tokenizer.decode(output_beam[0]))\n",
        "print(f\"\\nlog-prob: {logp: .2f}\")\n"
      ],
      "metadata": {
        "id": "D8sgET7pypMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sampling methods"
      ],
      "metadata": {
        "id": "6Fnq_JlR1mZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def softmax(logits,T=1):\n",
        "  e_x=np.exp(logits/T)\n",
        "  return e_x / e_x.sum()\n",
        "logits=np.exp(np.random.random(1000))\n",
        "sorted_logits=np.sort(logits)[: :-1]\n",
        "x=np.arange(1000)\n",
        "for T in [0.5,1.0,2.0]:\n",
        "  plt.step(x,softmax(sorted_logits,T), label=f'T={T}')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('Sorted token probabilities')\n",
        "plt.ylabel('Probability')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qkfwR-_R1mVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42);\n"
      ],
      "metadata": {
        "id": "hmHn1we91mTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_temp=model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                           temperature=2.0,top_k=0)\n",
        "print(tokenizer.decode(output_temp[0]))"
      ],
      "metadata": {
        "id": "5eRrikKw3nWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature=2.0 it gives random words and sentence losses its meaning beacuse it increases randomness"
      ],
      "metadata": {
        "id": "LKNMECJO4OuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_temp=model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                           temperature=0.5,top_k=0)\n",
        "print(tokenizer.decode(output_temp[0]))"
      ],
      "metadata": {
        "id": "A7fbJQmD3-vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature=0.5 it reduces random words and sentence gains its  full meaning beacuse it decreases randomness"
      ],
      "metadata": {
        "id": "rgaI05Xi3-sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-k and Nucleus Sampling(a.k.a top-p)"
      ],
      "metadata": {
        "id": "HRQqYljf4oTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42);\n"
      ],
      "metadata": {
        "id": "PUis_UEt4oQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "with torch.no_grad():\n",
        "  output=model(input_ids=input_ids)\n",
        "  next_token_logits=output.logits[:,-1,:]\n",
        "  probs=torch.softmax(next_token_logits, dim=-1).detach().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "74QUZx8F4pyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
        "\n",
        "axes[0].hist(probs[0], bins=np.logspace(-10, -1, 100), color=\"C0\", edgecolor=\"C0\")\n",
        "axes[0].set_xscale(\"log\")\n",
        "axes[0].set_yscale(\"log\")\n",
        "axes[0].set_title(\"Probability distribution\")\n",
        "axes[0].set_xlabel(\"Probability\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "#axes[0].grid(which=\"major\")\n",
        "\n",
        "axes[1].plot(np.cumsum(np.sort(probs[0])[::-1]), color=\"black\")\n",
        "axes[1].set_xlim([0, 10000])\n",
        "axes[1].set_ylim([0.75, 1.01])\n",
        "axes[1].set_title(\"Cumulative probability\")\n",
        "axes[1].set_ylabel(\"Probability\")\n",
        "axes[1].set_xlabel(\"Token (descending probability)\")\n",
        "#axes[1].grid(which=\"major\")\n",
        "axes[1].minorticks_on()\n",
        "#axes[1].grid(which='minor', linewidth='0.5')\n",
        "top_k_label = 'top-k threshold (k=2000)'\n",
        "top_p_label = 'nucleus threshold (p=0.95)'\n",
        "axes[1].vlines(x=2000, ymin=0, ymax=2, color='C0', label=top_k_label)\n",
        "axes[1].hlines(y=0.95, xmin=0, xmax=10000, color='C1', label=top_p_label, linestyle='--')\n",
        "axes[1].legend(loc='lower right')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "eNNn-_ZQ4pvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42);"
      ],
      "metadata": {
        "id": "SmgLxdPa6dfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_topk=model.generate(input_ids, max_length=max_length, do_sample=True,top_k=50)\n",
        "\n",
        "print(tokenizer.decode(output_topk[0]))"
      ],
      "metadata": {
        "id": "4K_vh1-x6db2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#top_k=50 ---> hyperparametr we can increase or decrease it based on our task"
      ],
      "metadata": {
        "id": "bQZGpN-56dZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42);"
      ],
      "metadata": {
        "id": "O9HaZ6fk7nKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_topp=model.generate(input_ids, max_length=max_length, do_sample=True,top_p=0.90)\n",
        "\n",
        "print(tokenizer.decode(output_topp[0]))"
      ],
      "metadata": {
        "id": "7JpK-r8p7QJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3f2qodj7QGD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}