{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xfJwAFBILCsY3fZy6zZik2Vjr-d65hRp",
      "authorship_tag": "ABX9TyMHDLWADOD07cuT6K7VFc3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/crack_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19tjB3Ipo_P9"
      },
      "outputs": [],
      "source": [
        "!unzip -q '/content/drive/MyDrive/Conglomerate Concrete Crack Detection.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def load_image_mask(image_path, mask_path):\n",
        "  image=tf.io.read_file(image_path)\n",
        "  image=tf.image.decode_jpeg(image, channels=3)\n",
        "  image=tf.image.resize(image, (256,256))\n",
        "  image=tf.cast(image, tf.float32) / 255\n",
        "\n",
        "  mask=tf.io.read_file(mask_path)\n",
        "  mask=tf.image.decode_png(mask, channels=1)\n",
        "  mask=tf.image.resize(mask, (256,256))\n",
        "  mask=tf.cast(mask, tf.float32) / 255\n",
        "\n",
        "  return image, mask\n",
        "\n",
        "def augment(image,mask):\n",
        "  if tf.random.uniform(())>0.5:\n",
        "    image=tf.image.flip_left_right(image)\n",
        "    mask=tf.image.flip_left_right(mask)\n",
        "\n",
        "  if tf.random.uniform(())>0.5:\n",
        "      image=tf.image.flip_up_down(image)\n",
        "      mask=tf.image.flip_up_down(mask)\n",
        "\n",
        "  if tf.random.uniform(())>0.5:\n",
        "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "\n",
        "  if tf.random.uniform(())>0.5:\n",
        "          image=tf.image.random_contrast(image,lower=0.9,upper=1.1)\n",
        "\n",
        "  return image, mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset(image_paths, mask_paths, batch_size=16, augment_data=False):\n",
        "  image_paths=sorted([os.path.join(image_paths, fname) for fname in os.listdir(image_paths)])\n",
        "  mask_paths=sorted([os.path.join(mask_paths, fname) for fname in os.listdir(mask_paths)])\n",
        "\n",
        "\n",
        "  dataset=tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "  dataset=dataset.map(load_image_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  if augment_data:\n",
        "    dataset=dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "  dataset=dataset.shuffle(buffer_size=1000) if augment_data else dataset\n",
        "  dataset=dataset.batch(batch_size)\n",
        "  dataset=dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "train_dataset= create_dataset(image_paths='/content/Conglomerate Concrete Crack Detection/Train/images', mask_paths='/content/Conglomerate Concrete Crack Detection/Train/masks',\n",
        "                              batch_size=16, augment_data=True)\n",
        "val_dataset=create_dataset(image_paths='/content/Conglomerate Concrete Crack Detection/Test/images', mask_paths='/content/Conglomerate Concrete Crack Detection/Test/masks',\n",
        "                           batch_size=16, augment_data=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "NhopwuaqrJc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "image_dir='/content/Conglomerate Concrete Crack Detection/Train/images'\n",
        "mask_dir='/content/Conglomerate Concrete Crack Detection/Train/masks'\n",
        "\n",
        "image_paths=sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir)])\n",
        "mask_paths=sorted([ os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)])\n",
        "\n",
        "idx=random.sample(range(len(image_paths)),4)\n",
        "\n",
        "plt.figure(figsize=(12,10))\n",
        "for i in range(len(idx)):\n",
        "  img, msk = load_image_mask(image_paths[idx[i]], mask_paths[idx[i]])\n",
        "\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(img)\n",
        "  plt.title('Image')\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(2,4,i+5)\n",
        "  plt.imshow(msk, cmap='gray')\n",
        "  plt.title('Mask')\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "5t6nLjbWrJZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(inputs, num_filters):\n",
        "  x=tf.keras.layers.Conv2D(num_filters, 3, padding='same')(inputs)\n",
        "  x=tf.keras.layers.BatchNormalization()(x)\n",
        "  x=tf.keras.layers.Activation('relu')(x)\n",
        "  x=tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "  x=tf.keras.layers.Conv2D(num_filters, 3, padding='same')(x)\n",
        "  x=tf.keras.layers.BatchNormalization()(x)\n",
        "  x=tf.keras.layers.Activation('relu')(x)\n",
        "  x=tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def encoder_block(inputs, num_filters):\n",
        "  x=conv_block(inputs, num_filters)\n",
        "  p=tf.keras.layers.MaxPool2D((2,2))(x)\n",
        "  return x, p\n",
        "\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "  x=tf.keras.layers.Conv2DTranspose(num_filters, (2,2), strides=2, padding='same')(inputs)\n",
        "  x=tf.keras.layers.Concatenate()([x, skip_features])\n",
        "  x=conv_block(x,num_filters)\n",
        "  return x\n",
        "\n",
        "\n",
        "def build_unet(input_shape):\n",
        "  inputs=tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  s1,p1=encoder_block(inputs,32)\n",
        "  s2,p2=encoder_block(p1,64)\n",
        "  s3,p3 = encoder_block(p2,128)\n",
        "  s4,p4=encoder_block(p3,256)\n",
        "\n",
        "  b1=conv_block(p4,512)\n",
        "\n",
        "  d1=decoder_block(b1,s4,256)\n",
        "  d2=decoder_block(d1,s3,128)\n",
        "  d3=decoder_block(d2,s2,64)\n",
        "  d4=decoder_block(d3,s1,28)\n",
        "\n",
        "  outputs=tf.keras.layers.Conv2D(1,1, padding ='same', activation='sigmoid')(d4)\n",
        "\n",
        "  model= tf.keras.models.Model(inputs, outputs, name='U-Net')\n",
        "  return model\n",
        "\n",
        "input_shape=(256,256,3)\n",
        "model=build_unet(input_shape)"
      ],
      "metadata": {
        "id": "fFdAyoRFyf6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# model.fit(train_dataset, epochs=2, validation_data= val_dataset)"
      ],
      "metadata": {
        "id": "tBmFWlO9yf3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(y_true, y_pred, smooth=1):\n",
        "  y_true=tf.cast(y_true, tf.float32)\n",
        "  y_pred= tf.cast(y_pred, tf.float32)\n",
        "\n",
        "  intersection= tf.reduce_sum(y_true * y_pred)\n",
        "  union= tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
        "\n",
        "  dice=(2. * intersection + smooth) / ( union + smooth)\n",
        "  return 1-dice\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "  bce=tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
        "  dice= dice_loss(y_true,y_pred)\n",
        "  return bce+dice\n",
        "\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "  y_true=tf.cast(y_true, tf.float32)\n",
        "  y_pred=tf.cast(y_pred>0.5, tf.float32)\n",
        "\n",
        "  intersection= tf.reduce_sum(y_true * y_pred)\n",
        "  union= tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
        "\n",
        "  dice=(2. * intersection + smooth) / ( union + smooth)\n",
        "  return dice\n",
        "\n",
        "model.compile(optimizer='adam', loss=bce_dice_loss, metrics=[dice_coef])\n",
        "\n",
        "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n"
      ],
      "metadata": {
        "id": "uDQnKW-N7R45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_overlay_samples(image_dir, mask_dir, num_samples=5, alpha=0.8):\n",
        "  image_paths=sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir)])\n",
        "  mask_paths=sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)])\n",
        "\n",
        "  sample_indices=random.sample(range(len(image_paths)), num_samples)\n",
        "\n",
        "  plt.figure(figsize=(8,num_samples*3))\n",
        "\n",
        "\n",
        "  for i , idx in enumerate(sample_indices):\n",
        "    img, msk = load_image_mask(image_paths[idx], mask_paths[idx])\n",
        "\n",
        "    img=img.numpy()\n",
        "    msk=msk.numpy()\n",
        "\n",
        "    msk_rgb= tf.concat([msk,msk,msk], axis=-1).numpy()\n",
        "\n",
        "    overlay=img.copy()\n",
        "\n",
        "    overlay[msk_rgb[:,:,0]>0.5]=[1.0,0,0]\n",
        "\n",
        "    blended=(1-alpha)* img+alpha*overlay\n",
        "\n",
        "    plt.subplot(num_samples,1,i+1)\n",
        "    plt.imshow(blended)\n",
        "    plt.title(f\"image{idx} with mask overlay\")\n",
        "    plt.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "tIra9KMF7R1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths='/content/Conglomerate Concrete Crack Detection/Train/images'\n",
        "mask_paths='/content/Conglomerate Concrete Crack Detection/Train/masks'\n",
        "\n",
        "\n",
        "\n",
        "visualize_overlay_samples(image_dir=image_paths,mask_dir=mask_paths,num_samples=5)\n"
      ],
      "metadata": {
        "id": "u8Az8uL1NE3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_predictions(model, image_dir, mask_dir, num_samples=5, alpha=0.4):\n",
        "    image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir)])\n",
        "    mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir)])\n",
        "\n",
        "    sample_indices = random.sample(range(len(image_paths)), num_samples)\n",
        "\n",
        "    plt.figure(figsize=(8, num_samples * 3))\n",
        "\n",
        "    for i, idx in enumerate(sample_indices):\n",
        "        # Load image and mask\n",
        "        img, msk = load_image_mask(image_paths[idx], mask_paths[idx])\n",
        "\n",
        "        # Get the model prediction (pred mask)\n",
        "        pred_mask = model.predict(tf.expand_dims(img, axis=0))[0]\n",
        "\n",
        "        # Prepare the masks and image\n",
        "        img = img.numpy()\n",
        "        msk = msk.numpy()\n",
        "\n",
        "        # Convert mask to RGB for overlay\n",
        "        msk_rgb = tf.concat([msk, msk, msk], axis=-1).numpy()  # Ensure msk_rgb has 3 channels\n",
        "        pred_rgb = tf.concat([pred_mask, pred_mask, pred_mask], axis=-1)  # Predicted mask as RGB\n",
        "\n",
        "        # Overlay: Image + (Ground truth mask * color)\n",
        "        gt_overlay = img.copy()\n",
        "        # Change here: Assign to all 3 color channels\n",
        "        gt_overlay[msk_rgb[:,:,0] > 0.5] = [1.0, 0, 0]  # Red color where mask is 1\n",
        "\n",
        "        # Overlay: Image + (Predicted mask * color)\n",
        "        pred_overlay = img.copy()\n",
        "        # Change here: Assign to all 3 color channels\n",
        "        pred_overlay[pred_rgb[:,:,0] > 0.5] = [0, 1.0, 0]  # Green color where predicted mask is 1\n",
        "\n",
        "        # Blended (actual ground truth overlay)\n",
        "        blended_gt = (1 - alpha) * img + alpha * gt_overlay\n",
        "\n",
        "        # Blended (predicted mask overlay)\n",
        "        blended_pred = (1 - alpha) * img + alpha * pred_overlay\n",
        "\n",
        "        # Display ground truth and predicted mask overlays\n",
        "        plt.subplot(num_samples, 2, 2*i + 1)\n",
        "        plt.imshow(blended_gt)\n",
        "        plt.title(f\"Ground Truth {idx}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.subplot(num_samples, 2, 2*i + 2)\n",
        "        plt.imshow(blended_pred)\n",
        "        plt.title(f\"Prediction {idx}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "g09iyrhdOqdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_predictions(\n",
        "    model=model,\n",
        "    image_dir=image_paths,\n",
        "    mask_dir=mask_paths,\n",
        "    num_samples=5\n",
        ")"
      ],
      "metadata": {
        "id": "ht7FHYb8PX8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbCNjWV4Paty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}