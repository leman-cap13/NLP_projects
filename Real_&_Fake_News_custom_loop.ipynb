{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsTTklFzFlmORhYr2UjpDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leman-cap13/my_projects/blob/main/Real_%26_Fake_News_custom_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload dataset"
      ],
      "metadata": {
        "id": "07Edye07QG9T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yTxrr0-zYeE"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "9I3IMmsyzhDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download razanaqvi14/real-and-fake-news"
      ],
      "metadata": {
        "id": "VOmVzj31zqHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/real-and-fake-news.zip', 'r')\n",
        "zip_ref.extractall()\n"
      ],
      "metadata": {
        "id": "EHPfBSCdz8FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qviq4n3nzuYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "8yp41DK_z0MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_true=pd.read_csv('/content/True.csv')"
      ],
      "metadata": {
        "id": "bFbw07blz4HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake=pd.read_csv('/content/Fake.csv')"
      ],
      "metadata": {
        "id": "_Su8Crc_0J03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_true"
      ],
      "metadata": {
        "id": "47NU841c0RIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake"
      ],
      "metadata": {
        "id": "L51vK2Vu0RoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_true.isna().sum()\n"
      ],
      "metadata": {
        "id": "_tvgVKKw0Tsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake.isna().sum()"
      ],
      "metadata": {
        "id": "c-xkRmQw0uyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hər dataframe-ə label əlavə et:\n",
        "df_true[\"label\"] = 1  # Real news\n",
        "df_fake[\"label\"] = 0  # Fake news"
      ],
      "metadata": {
        "id": "3ZNmbHi19Q3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_true, df_fake], ignore_index=True)"
      ],
      "metadata": {
        "id": "5I5Ak2js0yt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "enTT_O7V0yqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data-ı qarışdır (shuffle):\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "99hiVjaV1vnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "frac=1 – bütün datanı al\n",
        "\n",
        "sample() – random olaraq qarışdır\n",
        "\n",
        "reset_index(drop=True) – indexləri sıfırdan başlat"
      ],
      "metadata": {
        "id": "SsqKVRaG92Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ukjn_SRK93HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"] = df[\"title\"] + \" \" + df[\"text\"] # title ve text ikisinide istifade etmek isteyirem deye birlesdirdim\n",
        "X = df[\"content\"].values\n",
        "y = df[\"label\"].values"
      ],
      "metadata": {
        "id": "Pvg8kdCL94m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "gZqdKJKH_avI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"content\"].apply(lambda x: len(x.split())).describe()\n"
      ],
      "metadata": {
        "id": "I1m2X80jsqIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Low level api Custom loop"
      ],
      "metadata": {
        "id": "8QYUrPkr7S9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer,AutoModel\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7t2C_wXSRREU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Dghr6MmHR5ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "g-oP-DmuR5bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "id": "aDj6hW_uR5ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize xtrain xtest\n",
        "\n",
        "def tokenize_text(texts,labels,max_length=128):\n",
        "  tokens=tokenizer(\n",
        "      texts,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=max_length,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  tokens['labels']=torch.tensor(labels)\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "hXtlFYPnSLpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens=tokenize_text(X_train.tolist(),y_train.tolist())\n",
        "val_tokens=tokenize_text(X_test.tolist(),y_test.tolist())"
      ],
      "metadata": {
        "id": "IwSrGXs7SLmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hugging Face tokenizer() expects the text input to be one of:\n",
        "\n",
        "A str → for a single sentence\n",
        "\n",
        "A List[str] → for a batch of sentences\n",
        "\n",
        "A List[List[str]] → for tokenized input like [[\"I\", \"love\", \"NLP\"], [\"Transformers\", \"are\", \"great\"]]"
      ],
      "metadata": {
        "id": "vQacgfWQUP-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens"
      ],
      "metadata": {
        "id": "q24bQi4mSLj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make train dataset and valid dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset=TensorDataset(\n",
        "    train_tokens['input_ids'],\n",
        "    train_tokens['attention_mask'],\n",
        "    train_tokens['labels']\n",
        "                            )\n",
        "\n",
        "val_dataset=TensorDataset(\n",
        "    val_tokens['input_ids'],\n",
        "    val_tokens['attention_mask'],\n",
        "    val_tokens['labels']\n",
        ")"
      ],
      "metadata": {
        "id": "_xrlTq4YWx6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we need to craete data loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader=DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader=DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "37yrbW8YWx3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "shuffle=True → essential for training to break any patterns in data order\n",
        "\n",
        "shuffle=False → for validation, we keep it consistent"
      ],
      "metadata": {
        "id": "1eZhpEVPYtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader as a smart, iterable \"batch generator\" for your training data.\n",
        "\n",
        "Instead of giving all data at once to your model, DataLoader:\n",
        "\n",
        "Divides your dataset into batches\n",
        "\n",
        "Shuffles data (if told to)\n",
        "\n",
        "Feeds it to the model one batch at a time during training\n",
        "\n",
        "Can use parallel workers to speed up data loading"
      ],
      "metadata": {
        "id": "1xKMbCe8ZIY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make bert model\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model=BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "HUhOP6AmWx0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You don’t need to manually define:\n",
        "\n",
        "Embeddings\n",
        "\n",
        "Attention\n",
        "\n",
        "Final linear layer\n",
        "\n",
        "It’s all handled."
      ],
      "metadata": {
        "id": "BWmvb1HQZ6J0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We usually don’t need these values during standard training and evaluation because:( outpu_attention and output_hidden_states FALSE)\n",
        "\n",
        "They add extra outputs, which consume more memory.\n",
        "\n",
        "They slow down forward passes.\n",
        "\n",
        "Most loss and optimization routines don’t use them.\n",
        "\n",
        "Since your goal is fine-tuning for classification, you just want the logits output (final predictions), so you save resources by keeping them False."
      ],
      "metadata": {
        "id": "kvLP8uwIbVHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "l3o42Bv0Wxym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need to setup  Optimizer, Loss, and Scheduler\n",
        "\n",
        "loss_fn=torch.nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=2e-5)"
      ],
      "metadata": {
        "id": "ApI-51XibjOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learning rate scheduler\n",
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs=5\n",
        "num_training_steps=num_epochs*len(train_loader)\n",
        "lr_scheduler=get_scheduler(\n",
        "    name='linear',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "K9T48xWubjK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator=Accelerator()\n",
        "model, optimizer, train_loader, val_loader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_loader, val_loader, lr_scheduler\n",
        ")"
      ],
      "metadata": {
        "id": "Qdeq_PRDdn-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so far we prepare all we need for a custom loop\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar=tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for batch in train_loader:\n",
        "    input_ids=batch[0].to(device)\n",
        "    attention_mask=batch[1].to(device)\n",
        "    labels=batch[2].to(device)\n",
        "    batch=input_ids,attention_mask,labels\n",
        "    outputs=model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
        "    loss=outputs.loss\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    progress_bar.update(1)\n",
        "\n",
        "  model.eval()\n",
        "  preds=[]\n",
        "  true_labels=[]\n",
        "  for batch in val_loader:\n",
        "    input_ids=batch[0].to(device)\n",
        "    attention_mask=batch[1].to(device)\n",
        "    labels=batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "          batch=input_ids,attention_mask,labels\n",
        "          outputs=model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
        "    predictions=outputs.logits.argmax(dim=-1)\n",
        "    preds.extend(predictions.cpu().numpy())\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "3GnsXEuwbjIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tqdm** is a Python library that adds a visual progress bar to loops.\n",
        "When using PyTorch, model and data must both be on the same device (CPU or GPU).\n",
        "This line (batch={k:v.to(device) for k, v in batch.items()}) moves each tensor (input_ids, attention_mask, labels, etc.) in the batch to device."
      ],
      "metadata": {
        "id": "YmpgYSUehCv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch={\n",
        "  \"input_ids\": ...,\n",
        "  \"attention_mask\": ...,\n",
        "  \"labels\": ...\n",
        "}\n",
        "\n",
        "And **batch unpacks it as:\n",
        "model(input_ids=..., attention_mask=..., labels=...)\n"
      ],
      "metadata": {
        "id": "ZwBnh612hulf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer.step() updates the model weights to reduce the loss.\n",
        " loss.backward() computes gradients\n",
        " optimizer.step() applies them (i.e., makes the model learn)\n",
        " optimizer.zero_grad() resets gradients to zero for the next batch"
      ],
      "metadata": {
        "id": "DWkKSg-Zh299"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "argmax(dim=-1) returns the index of the max logit (i.e., predicted class) for each row.\n",
        "\n",
        "dim=-1 means apply argmax across the last dimension → across class scores (usually 0: fake, 1: real)."
      ],
      "metadata": {
        "id": "FI4u8nuOiNyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(true_labels, preds)\n",
        "print(f\"Epoch {epoch + 1}: Validation Accuracy = {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "COrZaeQPbjGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extractor"
      ],
      "metadata": {
        "id": "AFnzWFboQB_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#indi mene tokenler elde etmek lazimdi bunun ucun AutoTokenizer isledecem\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "model_ckpt = 'distilbert-base-uncased'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModel.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "M6FXrFp570xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding class\n",
        "def get_embedding(text):\n",
        "  inputs=tokenizer(text, return_tensors='pt',truncation=True,padding=True,max_length=512)\n",
        "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "  with torch.no_grad():\n",
        "    outputs=model(**inputs)\n",
        "  cls_embedding=outputs.last_hidden_state[:,0,:]\n",
        "  return cls_embedding.squeeze().cpu().numpy()"
      ],
      "metadata": {
        "id": "gqWW4g1NAQd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddinglerini gotur\n",
        "embeddings = []\n",
        "for text in df['content']:\n",
        "    emb = get_embedding(text)\n",
        "    embeddings.append(emb)"
      ],
      "metadata": {
        "id": "8SCoTsRICgD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X i update et\n",
        "X=np.array(embeddings)"
      ],
      "metadata": {
        "id": "XiiKFclgC1Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "7hZuPSx3JXxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y\n"
      ],
      "metadata": {
        "id": "9ihYmGfTJaE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trian test e bolmek ucun train_test_split istifade etdim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "hbVWrq8gJn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_clf=LogisticRegression()\n",
        "lr_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "g6JDLu1_I3yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_clf.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "B49yOnAMJqn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_clf.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "RwGGPPClJszI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(y_preds, y_true, labels):\n",
        "  cm=confusion_matrix(y_true, y_preds, normalize='true')\n",
        "  fig,ax=plt.subplots(figsize=(6,6))\n",
        "  disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "  disp.plot(cmap='Blues', values_format='.2f', ax=ax, colorbar=False)\n",
        "  plt.title('Normalized confusion matrix')\n",
        "  plt.show()\n",
        "y_preds=lr_clf.predict(X_test)\n",
        "plot_confusion_matrix(y_preds,y_test, labels=['Real', 'Fake'])"
      ],
      "metadata": {
        "id": "flDEhMVBJu59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = df['content'][0]\n",
        "\n",
        "# Yenidən tokenləşdir:\n",
        "inputs = tokenizer(sample_text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Token id-ləri geri çevirmək:\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "q11pnaNSKMVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_preds))"
      ],
      "metadata": {
        "id": "31iyNCqqKswg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fine tuning"
      ],
      "metadata": {
        "id": "wDBfO9SKQID-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tuning\n",
        "\n",
        "from transformers import  AutoModelForSequenceClassification\n",
        "num_labels=2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)"
      ],
      "metadata": {
        "id": "0L94K2AGLF0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics hazirlayaq\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "def compute_metrics(pred):\n",
        "  labels=pred.label_ids\n",
        "  preds=pred.predictions.argmax(-1)\n",
        "  f1=f1_score(labels, preds, average='weighted')\n",
        "  acc=accuracy_score(labels, preds)\n",
        "  return {'accuracy': acc, 'f1': f1}"
      ],
      "metadata": {
        "id": "mJ55AgFmLFwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "1bHyvZo8Ma7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "model_name=f'{model_ckpt}-finetuned-fake_true_news'\n",
        "training_args=TrainingArguments(\n",
        "    output_dir=model_name,  #main\n",
        "    num_train_epochs=2,  #main\n",
        "    learning_rate=2e-5,   #main\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy='epoch',\n",
        "    disable_tqdm=False,\n",
        "    push_to_hub=True,\n",
        "    log_level='error'\n",
        ")"
      ],
      "metadata": {
        "id": "HiJbqK3BM-vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"content\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "SN314h30P-7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"text\": train_texts,\n",
        "    \"label\": train_labels\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"text\": val_texts,\n",
        "    \"label\": val_labels\n",
        "})\n"
      ],
      "metadata": {
        "id": "tMei8JZ2NomT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )"
      ],
      "metadata": {
        "id": "smaT1s8jPDt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.remove_columns([\"text\"])\n",
        "val_dataset = val_dataset.remove_columns([\"text\"])"
      ],
      "metadata": {
        "id": "BtIs8dnQPJjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer=Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "5DzGFVgXNVfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "r-8zzpR3N5MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output=trainer.predict(val_dataset)"
      ],
      "metadata": {
        "id": "CaXz8Fi_OHAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_output"
      ],
      "metadata": {
        "id": "gE7She3RafSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(commit_message='Training completed')"
      ],
      "metadata": {
        "id": "uh7rKVC4as2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}